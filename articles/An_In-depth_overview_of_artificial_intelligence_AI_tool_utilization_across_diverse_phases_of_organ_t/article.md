# An In-depth overview of artificial intelligence (AI) tool utilization across diverse phases of organ transplantation.

**Authors:** Arjmandmazidi S, Heidari HR, Ghasemnejad T, Mori Z, Molavi L, Meraji A, Kaghazchi S, Mehdizadeh Aghdam E, Montazersaheb S.

**Journal:** J Transl Med (2025)


## Abstract

No abstract available.

## Graphical Abstract



## Introduction

Artificial intelligence (AI) refers to the utilization of machine learning (ML) and deep learning (DL) algorithms to perform tasks usually performed by humans, like reasoning, discovering, generalizing, and learning from experiences and past events [ 1 , 2 ]. AI has numerous applications in medicine, revolutionizing diagnostics, treatment planning, and patient care. Machine learning algorithms can analyze vast amounts of medical data to detect diseases early, predict patient outcomes, and personalize treatments. AI-powered imaging tools assist radiologists in identifying abnormalities with greater accuracy, while predictive analytics help hospitals manage resources efficiently. In surgery, robotic-assisted systems enhance precision and reduce recovery times. Additionally, AI-driven chatbots and virtual assistants improve patient engagement and streamline administrative tasks. These advancements not only enhance efficiency but also contribute to better clinical decision-making, ultimately improving patient outcomes across various medical fields [ 3 ].Clinical practice and health systems have benefited from this technology, from laboratory practice to replacing humans in the surgery rooms [ 4 – 7 ]. Currently, the realm of AI has reached a point where it can autonomously detect diabetic retinopathy, without the need for confirmation by ophthalmologists. Moreover, AI applications are venturing into the physical domain, showcasing advancements such as robotic prostheses, systems offering support for physical tasks, and mobile manipulators that play a pivotal role in telemedicine delivery [ 8 ]. Various endoscopy manufacturers have successfully launched their AI devices in the market, having secured regulatory approval in Europe and Asia [ 9 ]. Organ transplantation is no exception to this rule. To realize the significance of the improvement and update of the organ transplantation field, there are some facts given from the CDC website which show how many human lives depend on a suitable organ received; in 2022 about 100,000 patients were added to the waiting list of organ transplantation each day while only 15,000 deceased or alive organ donors are available annual in the United States. The most commonly transplanted organs in the United States are the kidney, liver, heart, lungs, pancreas, and intestines [ 10 ]. As the demands increase over time, the organ availability decreases [ 11 ]. Given that artificial intelligence has demonstrated its significant value in numerous aspects of healthcare, including gastroenterology, endoscopy, medical diagnosis, medical statistics, and genetics, it stands to reason that organ transplantation could also benefit from AI applications [ 12 – 14 ]. Several studies showed that AI had conquered areas like organ allocation and donor-recipient matching, transplant oncology, and immunosuppression regimes in organ transplantation [ 15 – 19 ].

The organ transplantation procedure comprises three main steps: pre-transplant evaluation of donors and recipients, transplantation surgery, and post-transplant management. AI can facilitate the recipient-donor matching through the analysis and integration of information from histopathological reports, laboratory test outcomes, radiological characteristics, and patient demographic data. In addition, AI can aid robotic guided surgery and optimize post-transplantation regimens and evaluate complications [ 20 ]. For instance, a considerable body of literature delves into the utilization of AI in various facets of kidney transplants. Some researchers have employed Machine Learning (ML) to anticipate the bioavailability of tacrolimus in the immediate post-transplant phase and assess the likelihood of post-transplant diabetes mellitus based on the genetic phenotypes, age, gender, and body mass index [ 21 ]. Accordingly, in this review we aimed to gather information regarding the application of AI in the different steps of organ transplant, focusing more on the strategies and tools regarding different stages of transplant procedures.

## AI types used in transplant process

There are various types of AI tools using in the different areas of medicine. They mainly include Machine learning, Deep learning, and Neural Networks. The most common AI methodologies regarding these three strategies are summarized in Table 1 . However, Machine Learning, as the most common branch of AI, is explained more in the following section.

Table 1 Different AI models used in organ transplant Category AI Model Brief explanation Neural Networks & Deep Learning Deep Learning a subset of machine learning that focuses on using neural networks with many layers (deep neural networks) to model and understand complex patterns in data. It is particularly powerful for tasks. Deep Neural Network (DNN) a type of artificial neural network with multiple layers of neurons between the input and output layers. These layers are known as hidden layers, and the depth of the network refers to the number of these hidden layers. DNNs are a subset of machine learning models and are a key component of deep learning, which is a branch of artificial intelligence DeepSurv a deep learning-based approach for survival analysis, specifically designed to predict the time to an event (such as failure or death) and the associated risk based on complex, high-dimensional data. It extends traditional survival models by leveraging the power of deep neural networks to handle complex relationships and interactions between predictors Generative Adversarial Networks (GAN) a type of deep learning architecture used for generating new data samples that resemble a given dataset. Introduced by Ian Goodfellow and his colleagues in 2014, GANs consist of two neural networks that compete with each other in a process that drives them to improve over time. Multilayer Perceptron (MLP) a type of artificial neural network used for various machine learning tasks, including classification and regression. It consists of multiple layers of nodes (neurons) arranged in a layered architecture. Each layer performs a specific function, and the network learns by adjusting weights through training Ensemble Methods Ensemble Learning By merging multiple decision trees into a unified model, the strengths and weaknesses of individual predictions are effectively utilized Random Forest A collection of independent decision trees with superior accuracy compared to the cumulative accuracy of each individual tree AdaBoost AdaBoost (Adaptive Boosting) is a popular ensemble learning technique designed to improve the performance of machine learning models by combining the predictions of multiple weak learners to create a strong classifier. Random Survival Forests a machine learning technique used for analyzing and predicting time-to-event data, often referred to as survival data. This method is an extension of the Random Forest algorithm, which is commonly used for classification and regression tasks Extreme Gradient Boosting (XGBoost) a highly efficient and scalable machine learning algorithm for supervised learning problems, particularly for classification and regression tasks. XGBoost is known for its ability to handle large datasets and its high predictive accuracy. Traditional Machine Learning Support Vector Machine (SVM) are a powerful and versatile class of supervised learning algorithms primarily used for classification tasks, but they can also be applied to regression problems. Logistic Regression a statistical model used for binary classification tasks. Despite its name, logistic regression is used for classification rather than regression. It estimates the probability of a binary outcome based on one or more predictor variables. Multivariable Logistic Regression Multivariable Logistic Regression (often referred to as Multivariate Logistic Regression or simply Logistic Regression with multiple predictors) is an extension of logistic regression that includes more than one predictor variable. Cox Proportional Regression Cox Proportional-Hazards Regression (often simply called Cox Regression) is a statistical method used for analyzing survival data and understanding the effect of explanatory variables on the time until an event occurs. Developed by Sir David Cox in 1972, it is widely used in medical research, epidemiology, and other fields dealing with time-to-event data. Survival Tree type of decision tree specifically designed for analyzing survival data, which involves time-to-event outcomes. They are used to predict the time until an event occurs (such as failure, death, or relapse) based on various predictor variables. Memetic Pareto Evolutionary NSGA-II an advanced multi-objective optimization algorithm. It combines the strengths of memetic algorithms and NSGA-II to efficiently solve complex optimization problems involving multiple conflicting objectives. MoRAL-AI MoRAL-AI (Model of Responsibility for AI) is a framework designed to address the ethical and responsible development and deployment of artificial intelligence systems. It focuses on integrating ethical considerations and responsible practices into AI models and processes Optimization & Evolutionary Algorithms Adaptive-network-based Fuzzy Inference System a type of artificial neural network that combines fuzzy logic with neural network learning capabilities. ANFIS is used for modeling complex, non-linear systems and can be employed in various applications such as prediction, classification, and control Decision Tree a popular machine learning and statistical technique used for classification and regression tasks. They represent a model that makes decisions based on a series of binary or categorical splits in the data, visualized as a tree structure. Each node in the tree represents a decision based on a feature, and each branch represents the outcome of that decision Fuzzy Logic & Inference Systems Conditional Inference Tree a type of decision tree designed to avoid some common issues in traditional decision tree algorithms, such as bias towards variables with more categories or continuous variables. They use statistical tests to determine the best split points, ensuring that splits are selected based on statistical significance rather than just algorithmic criteria. Decision Trees & Pruning Methods Case-Based Reasoning an approach within artificial intelligence and machine learning where new problems are solved based on the solutions of similar past problems Case Retrieval Nets a sophisticated method used in case-based reasoning (CBR) to efficiently retrieve relevant cases from a large case base. Case-Based Reasoning & Retrieval Spreading Activation Algorithm a family of techniques used in artificial intelligence, cognitive science, and information retrieval to find and rank related concepts or nodes within a network. These algorithms simulate how human memory might work, by “activating” certain nodes and allowing this activation to spread to related nodes, thereby identifying relevant or related information. Reinforcement Learning Enhances the likelihood of attaining a goal within a specific circumstance or setting. Finite Impulse Response (FIR) a type of digital filter used in signal processing. FIR filters are characterized by their response to an impulse input that settles to zero in a finite amount of time. They are widely used due to their stability and linear phase properties. Reinforcement Learning Lund DLTransplant Algorithm a specific set of criteria and scoring system used to prioritize patients for liver transplantation. It is particularly focused on pediatric liver transplants and aims to optimize outcomes by effectively allocating liver donations based on the urgency of need and potential benefit. Signal Processing MUSA-UNet an advanced variation of the UNet architecture specifically designed for medical image segmentation tasks. It incorporates several enhancements to improve performance and accuracy, particularly in challenging medical imaging scenarios Specialized Models Binary Thresholding a simple image processing technique used to segment an image into two distinct regions based on pixel intensity values. It is commonly used in various applications, including image binarization, object detection, and feature extraction. Multiple Regression Models a statistical technique used to understand the relationship between one dependent variable and two or more independent variables. This method extends simple linear regression, which involves only one independent variable, to analyze and predict the dependent variable based on the combined influence of several predictors

## Machine learning (ML) as important tool for organ transplant

ML constitutes a pivotal domain within AI, characterized by a suite of automated computational algorithms. At its core, ML employs algorithms to repetitively refine predictive and classificatory capacities based on prior data instances, enabling capable of extrapolation to novel datasets [ 22 – 24 ]. In the medical field, datasets are growing rapidly, including millions of patient profiles collected from various sources like imaging, electronic health records, telemedicine, and genetic databases. Some algorithms can be applicable for managing computational workflows in organ transplant dataset generation. As an example Khaledian et al. used a hybrid Particle Swarm Optimization and Simulated Annealing algorithm (PSO–SA) as energy-efficient and deadline-aware workflow scheduling algorithm in the fog and cloud environment [ 25 ].

Nonetheless, the widespread challenge of data quality persists as a serious obstacle highlightening the need for precise data cleansing efforts. This is a demanding process that aims to correct inconsistencies and ensure accuracy, relevance, and coherence within datasets. This task becomes even more important when combining different data sources, as complex and disordered data can hinder AI innovation. In the past, this process depended on manual work, but modern advancements now provide smart data-cleaning tools with advanced algorithms. Some ML based tools can also be used for feature selection and medical data classification. As an example manta ray foraging optimizer-based SVM is developed for giving insights into feature selection in complex datasets [ 26 ]. The effectiveness of the suggested method is evaluated and contrasted with four algorithms based on Support Vector Machines (SVM) using eight benchmark datasets. Furthermore, the method is tested on a Covid-19 disease dataset. The outcomes of these experiments demonstrate the method’s strong capability in identifying suitable SVM parameters and its satisfactory performance in addressing the challenge of feature selection [ 26 ].

Some tools employ discriminatory rule sets to fix deviations within expansive datasets, limiting the time and resources required to uphold the integrity and precision of medical databases. Recent scholarly inquiries have also proposed novel models fitted to adjust irregular medical records and determine complicated feature interdependencies, improving individualized healthcare prognosis [ 27 – 31 ]. Fundamentally, an algorithm represents an in-depth and defined set of rules that compose sequential operations. Within ML, algorithms separately assemble insights from data without human intervention, leveraging constant adjustment and adaptation mechanisms. These algorithms decrypt latent patterns by analyzing dataset nuances, clarifying consequential conclusions essential for informed decision-making processes [ 32 , 33 ]. ML encompasses three main strategies: [ 1 ] supervised learning, entailing the manual mapping of observational features to known outcomes; [ 2 ] unsupervised learning, focused on uncovering intrinsic patterns within unlabeled data; and [ 3 ] reinforcement learning, which requires training ML models within interactive environments, facilitating sequential decision-making through constant trial and error, guided by ongoing feedback [ 23 ]. In the following sections, three main phases of organ transplant including Pre-Transplantation, Transplant Surgery and, Post-Transplant steps were covered regarding possible applied AI tools in each area. Whenever possible, the classified tools commonly used in different organ transplants are discussed separately. Figure 1 depicted some of AI applications in solid organ transplants.

Fig. 1 Applications of AI in the realm of solid organ transplant

## AI tools in pre-transplantation step



## Organ allocation and predictive prioritizing modeling for optimizing waitlists

During the pre-transplant phase, individuals being considered for organ transplantation undergo thorough assessments that cover medical, societal, and economic factors. The current standards for evaluation and placement on the transplant list are based on “clinical judgment” and “generalized heuristics”. Transplant candidates with increased medical complexity, characterized by older age, metabolic risk factors, and cardiovascular comorbidities, face higher risks of illness and death both while waiting for transplantation and after the procedure. This increased risk is linked to various complications, including infections, cancers, and adverse effects from medications. Consequently, allocating organs to recipients who can derive most optimal benefits from transplantation with the least possible risk is a significant challenge [ 34 – 37 ].

Based on prior research, the utilization of “extended criteria donors (ECD),” identified as donors aged 60 or older or those over 50 with a history of hypertension, creatinine > 1.5 mg/dL, or death from stroke, has proven advantageous for older patients and individuals in regions with prolonged waiting times. However, over the past 15 years, the landscape has evolved into greater complexity, with donors exhibiting more adverse characteristics and recipients bearing increased comorbidity burdens. This example, specific to kidney transplantation, holds relevance for other solid organ transplantations [ 38 ]. To address these challenges, the development of more sophisticated risk calculators incorporating recipient, donor, and center-specific characteristics can offer personalized risk assessments for each patient. Such an approach aims to optimize organ utilization, reduce discards, and decrease waitlist mortality [ 39 ]. In this regard, the optimization algorithms of AI techniques can be helpful for organ allocation purposes, Shehab et al. presents an overview of black widow optimization (BWO), a recently developed nature-inspired optimization algorithm. The review examines the primary components of BWO, discussing its advantages and limitations. It also explores the evolution of BWO, its various iterations, and its effectiveness across diverse fields and applications, including the management of medical data [ 40 ].

Harnessing the capabilities of ML, analysts can navigate through vast, complex, and heterogeneous datasets, yielding refined insights and highly sophisticated predictive models. In the medical domain, where complex datasets abound, ML techniques have emerged as potent tools, looking after the development of pivotal predictive models poised to enhance clinical practice across diverse medical disciplines [ 41 ]. For instance, visualization prediction of the 10-year survival rate of kidney grafts was performed as an complicated attempt wherein the algorithm is presented with a rich database containing countless variables, ranging from recipient demographics to medical histories. Each instance, or kidney transplant, is precisely labeled based on its eventual outcome—survival or failure by the decade mark. Utilizing this information, the system accurately identifies the underlying relationship between input variables and their corresponding outcomes. With this newfound understanding, the algorithm generates a predictive framework capable of estimating results for unfamiliar inputs, extending beyond the scope of its initial training dataset [ 42 ]. Existing organ allocation policies often hinge upon a limited arrangement of criteria, primarily centered on recipient need and donor-recipient compatibility. Yet, an optimal allocation framework necessitates a broader consideration of factors, including those influencing waitlist mortality (list of patients who are waiting for an organ transplant and are at risk of dying while waiting). ML, with its capacity to identify significant variables from both donor and recipient datasets, presents a promising opportunity for enhancing the sophistication of organ allocation systems. By probing complex, nonlinear relationships within the data, ML can illuminate novel insights, thereby facilitating more nuanced and intelligent organ allocation practices [ 43 ]. Other ML-based studies have combined donor and recipient traits to enhance post-transplant outcomes rather than solely focusing on waitlist mortality.

On the other hand, Artificial Neural Networks (ANN) mimic the human brain’s behavior, enabling algorithms to identify patterns and tackle common issues in AI, ML, and DL. ANN models employ neurons to process input features and generate output without explicit rules.

Additionally, technology-driven algorithms introduced by the United Network for Organ Sharing (UNOS) aim to offer organs to the centers likely to utilize them precisely. UNOS piloted the Organ Offer Explorer tool to compare new offers with prior accepted organs by surgeons, forwarding only compatible offers to transplant programs [ 44 ]. This decision-making aid relies on past acceptance behavior data, reorganizing organ offers, and lightening burdens, particularly amid recent kidney allocation reforms, which have heightened organ placement complexity [ 45 – 47 ].

Various ML tools exist to aid surgeons in understanding novel situations and their impact on transplant procedures. Amid the backdrop of the COVID-19 pandemic, a ML model has emerged to discern the potential benefits or risks associated with kidney transplants during this crisis. This study shows how adaptable machine learning (ML) is in rapidly changing clinical and social settings, helping to provide evidence-based care without relying heavily on traditional clinical trials. In pre-transplant assessments, AI has the potential to improve how we evaluate candidates, accept donors, and educate patients [ 48 – 51 ]. Techniques like transfer learning address data shortages by using prior knowledge to perform new tasks. For example, a transfer learning model can use public medical records to identify relevant medical features for tasks like assessing COVID-19 prognosis or predicting mortality in end-stage renal disease [ 52 ].

Some studies have developed models of using ML techniques in organ allocation. Hsich et al.. conducted a study in 2019 focusing on the mortality of adults awaiting heart transplants, using the Scientific Registry of Transplant Recipients database spanning from 2004 to 2015. They employed Random Survival Forests to identify critical variables influencing waitlist mortality [ 53 ]. Medved et al.. devised the Lund DLTransplant Algorithm (LuDeLTA), which simulates the heart allocation process using a discrete event model and a neural network algorithm [ 54 ].

Moreover, there are several studies designing AI models for pre-transplant different types of predictions. Here are some examples. Ayllón et al. [ 55 ] designed an ANN model in 2018, leveraging King’s College Hospital patient data to predict 3- and 12-month survival rates. Dorado-Moreno et al. [ 56 ]. utilized data from seven Spanish hospitals and King’s College Hospital (UK) in 2017 to create a dynamically weighted evolutionary ordinal neural network, addressing imbalances in liver transplantation. Placona et al. [ 57 ]. developed a model in 2020 to predict the likelihood of a delay or discard of adult deceased kidney donors between 2010 and 2018, utilizing donor free text data. These studies exemplify the diverse applications of ML in optimizing organ allocation processes.

## Optimizing donor-recipient matching

From a purely measured standpoint, transplantation can be broken down into several problems where the donor’s characteristics must be combined with the recipient’s variables to achieve one of three results: both the graft and recipient survive, the graft is lost, or both the graft and recipient are lost. The allocation of organs is closely related to donor-recipient matching. Despite thorough analysis and modification, traditional donor-recipient matching models still require improvement and could profit from AI. Donor selection is a complex, multifaceted decision influenced by donor and recipient factors as well as matching considerations. Regrettably, an ideal donor-recipient matching system remains elusive due to inconsistent evidence and unreliable endpoints.

A major challenge for researchers in classifying donor-recipient matching data is selecting the most appropriate classifiers for the challenge. Some classifiers are derived from biostatistics, others from AI and data mining (like rule-based classifiers or decision trees), while some use connectionist approaches (ANNs) or cooperative approaches, and others utilize regression or clustering approaches [ 58 ].

AI models should offer two elucidations: support decision-making with existing metrics based on logistic regression and enhance likelihood. While hundreds of classifiers could handle this issue, not all are effective for donor-recipient pairing. Assigning a donor to a candidate on the waiting list involves managing numerous variables related to the donor, recipient, logistics, and perioperative factors (Fig. 2 ). There are several algorithms for clustering and managing these variables. In operational analysis and Artificial Intelligence fields, meta-heuristics employ diverse search techniques to tackle complex real-world challenges, such as data clustering problems and traditional optimization issues. Abualigah et al. introduces an innovative approach that combines elements from intelligent optimization algorithms to address a range of problems requiring sophisticated methods. The proposed technique, named GNDAOA, incorporates three primary components: Arithmetic Optimization Algorithm (AOA), Generalized Normal Distribution Optimization (GNF), and Opposition-based Learning strategy (OBL). These elements are integrated using a novel transition mechanism, which coordinates the execution of the various methods during the optimization process. This approach aims to overcome the principal limitations of the original techniques [ 59 ].

Experience from the team can indirectly infer some of the latter variables. Two AI model groups used in donor-recipient matching are ANNs and random forests (RF) [ 60 ]. In liver donor-recipient matching, ANNs, which are biologically inspired computational networks, consist of input, hidden (in most cases), and output layers. They excel at identifying complex patterns that clinicians might miss and can produce highly accurate predictions based on the data they are trained on [ 61 ].

Fig. 2 Using artificial intelligence in the area of donor recipient matching considerations and decision making

However, ANNs are highly sensitive to database quality and essentially function as black-box models where all variables are significant. These models do not provide clear insights into the weight of each variable [ 60 ]. Conversely, RF builds decision trees that is effective with small cohorts. They can select top variables like logistic regression but struggle with large databases due to the high number of decision trees produced, making them impractical. Their performance can decay when faced with numerous features and relatively weak signals. Despite their complexity, RF algorithms offer several advantages. They can effectively manage non-linear variables, withstand noise in data, and allow for easy fine-tuning and parallel processing. Additionally, these algorithms incorporate an initial step to prioritize features, thereby reducing the variable space. The construction of each tree within a random forest relies on a random subset of observations, typically utilizing either a bootstrap sample or a portion of the original dataset [ 62 , 63 ].

Biostatistics-based metrics predict the most likely outcomes (survival, the majority class) well but are poor at foreseeing the least likely outcomes (loss, the minority class). This is where ANNs can benefit significantly, as they handle numerous variables and consequences, allowing them to generalize in the minority class. Another benefit of ANNs is their ability to foretell multiple outcomes. Using the same dataset, multi-objective ANNs can independently guess the probabilities of survival and loss. However, ANNs do not provide information on the factors involved in their computational decisions. A computer-driven allocation based solely on statistical likelihoods would always assign the donor to the candidate with the highest survival probability, regardless of the recipient’s severity. Therefore, it is necessary to set conditions for the computer to make donor assignments based on a combination of numerical probabilities and clinical judgment through a rule-based system, allowing the computer to adapt its decisions to the candidate’s significance and resolve ties with similar probabilities.

In 2014, Bricen˜o et al. utilized 64 donor and recipient variables from 1003 liver transplants across 11 Spanish centers. They calculated simple and multiple regression models and ANN formulae for each donor-recipient pair for two non-complementary probability models of 3-month graft survival and loss: a positive-survival (NN-CCR) and a negative-loss (NN-MS) model. They used the Neural Net Evolutionary Programming (NNEP) algorithm to cultivate the NN models. They validated ANNs against other scores using receiver-operating curves (ROC), indicating the predominance of ANNs in donor allocation over biostatistics-based prioritization scoring. Due to its rule-based system, the model excellently assigned patients with higher MELD scores [ 64 ].

In 2013, Cruz-Ramírez et al. used the memetic Pareto evolutionary non-dominated sorting genetic algorithm 2 (MPENSGA2), a multi-objective evolutionary algorithm, to educate radial basis function neural networks, assessing model performance using accuracy and minimum sensitivity measurements. The neural network models from the Pareto fronts helped develop a rule-based system aiding medical experts in organ allocation. This system is objective, avoiding biases from medical experts, though an expert must still make the final decision [ 17 ]. For RF in liver transplantation, the original dataset is split into training and test sets. About two-thirds of the donor-recipient pairs are used for training (bootstrap sample) with the remaining pairs forming the test set. This methodology, known as the estimation of errors out-of-the-bag, certifies no overlap between training and test sets [ 62 ].

In order to compare the traditional methods for prediction modeling such as MELD scores and novel AI tools, we gather the information in Table 2 . As shown in Table 2 , AI-based methods are generally advantageous regarding prediction accuracy, data processing, personalization and Real-Time adaptability. However, traditional clinical scoring systems shows more explainability, integration with clinical workflow, regulatory approval and validation and has better cost and implementation.

Table 2 Comparison between AI-based prediction models and traditional clinical scoring systems Features of prediction method AI-Based Models Traditional Models Example Models Artificial Neural Networks (ANNs), Random Forest, XGBoost, DeepSurv MELD (Model for End-Stage Liver Disease), SOFA (Sequential Organ Failure Assessment), APACHE (Acute Physiology and Chronic Health Evaluation) Prediction Accuracy Higher accuracy due to deep learning and large dataset analysis Lower accuracy, relies on predefined clinical scores Data Processing Can analyze high-dimensional data (genomics, imaging, EHRs, biometrics) Limited to structured clinical variables and statistical models Personalization Tailors predictions based on patient-specific features using adaptive learning More generalized risk assessment using population-based scores Real-Time Adaptability Continuously updates with new data and improves over time Static models that require manual recalibration Explainability Often criticized as a “black box”; some models (e.g., SHAP-based) improve interpretability More transparent and widely understood by clinicians Integration with Clinical Workflow Requires advanced IT infrastructure and clinician training Already integrated into routine clinical decision-making Regulatory Approval & Validation Needs extensive validation (e.g., FDA approval) due to complexity and variability Established and widely accepted in practice Cost & Implementation High initial investment in AI infrastructure, expertise, and data management Lower cost, as traditional models are already in use

## Transplant pathology

AI has demonstrated remarkable efficacy in image processing, particularly in handling complex information in medical images. Pathological slides, rich in structured and unstructured data, often require expert interpretation. However, the insufficiency of trained pathologists poses a challenge. Here, AI intervenes to extract, process, analyze, and even learn from this wealth of information to guide therapy and enhance diagnostic accuracy [ 1 , 65 ]. Furness et al. [ 66 ] developed an ML algorithm in 1999 that surpassed expert pathologists in diagnosing acute kidney allograft rejection. Tong et al. [ 67 ]. predicted heart rejection using deep neural networks and histopathological whole-slide imaging, achieving superior accuracy compared to manual assessment by examining 43 patients. Advancements in computing power, data storage, and network speed have led to more efficient image analysis. Hermsen et al. [ 68 ] successfully implemented a DL algorithm to segment kidney biopsies into anatomical components, utilizing convolutional neural networks for multiclass segmentation of digitized tissue sections. Furthermore, Perez-Sanz et al. [ 69 ] developed a computer vision-based application in 2021 for rapid and objective quantification of macrovesicular steatosis in liver histopathological slides. These advancements highlight the transformative potential of AI in medical imaging and pathology. Since pathology is a common area for AI tool applications in medical transplants, individual tools for heart, lung, kidney, and liver transplants are discussed separately in the following sections.

## Heart transplant

Endomyocardial biopsy (EMB) with histopathology grading remains the gold standard for diagnosing cardiac allograft rejection, a critical concern in heart transplantation. Recent reports have addressed key issues, such as the reliability of grading systems, challenges in biopsy interpretation, the natural history of rejection, the need for late surveillance biopsies, and the effectiveness of noninvasive techniques in rejection diagnosis or prediction [ 70 ].

Giuste et al. [ 71 ] pioneered the development of a DL model to automatically quantify rejection risk within digital images of biopsied tissue. Their approach employed explainable synthetic data augmentation, utilizing progressive and inspirational Generative Adversarial Networks (GANs). By generating high-resolution synthetic images that feature rejection signs, they enhanced the performance of their allograft rejection classifier model. Similarly, Seraphin et al. [ 72 ] conducted a comprehensive study involving 1079 histopathology slides from 325 patients across three transplant centers in Germany. They trained an attention-based deep neural network to predict rejection in the primary cohort. They evaluated its performance through cross-validation and deployment to additional cohorts. Their findings showed the potential of AI in detecting patterns of cellular transplant rejection, even with training on relatively small cohorts.

In a separate endeavor in 2022, Lipkova et al. [ 73 ] introduced a deep learning-based artificial AI system designed for the automated assessment of gigapixel whole-slide images obtained from endomyocardial biopsy (EMB). Their pre-trained deep residual Convolutional Neural Network (CNN) model exhibited remarkable performance in detecting, subtyping, and grading cardiac allograft rejection, boasting a notable Area under the Curve (AUC) of 0.962. These groundbreaking studies highlight the significant progress made in leveraging AI to advance the diagnosis and management of cardiac allograft rejection. Figure 3 demonstrates the application of machine learning algorithms to interpret Endomyocardial biopsy (EMB) slide data, obtained before and after transplantation, to determine cardiac allograft rejection.

Fig. 3 Using machine learning models for interpreting the date from Endomyocardial biopsy (EMB) slides, both pre and post-transplant, for establishing cardiac allograft rejection

## Lung transplant

Despite advancements in immunosuppressive therapies and drugs, a significant proportion of lung transplant recipients encounter at least one treated acute rejection episode within the initial year post-transplantation, as reported by the International Society of Heart and Lung Transplantation registry [ 74 ]. However, the reproducibility of diagnosing acute cellular rejections (ACR) remains variable, even among seasoned transplant pathologists. ML has emerged as a valuable tool for developing predictive models, aiding clinicians in making more informed and reliable decisions. The abundance of data collected throughout the lung transplant process enables the extraction of hidden patterns through ML methods [ 75 ]. These approaches promise to improve patient outcomes and refine clinical management strategies in lung transplantation.

Gholamzadeh et al. [ 76 ] organized a comprehensive systematic review utilizing five electronic databases from January 2000 to June 2022. Their study investigated classical ML-based techniques to enhance lung transplantation outcomes and mitigate complications. The resulting prediction models offer clinicians valuable insights, empowering them to make more informed and reliable decisions by digging out novel knowledge from the vast reservoir of lung transplantation data. Meanwhile, Davis et al. [ 77 ] Looked into the detection of ACR in lung transplant biopsies using AI. Notably, their AI model achieved a remarkable validation accuracy of 95% in distinguishing the vascular component of ACR from normal alveolar lung tissue. Their findings present encouraging prospects for identifying ACR, a precursor to chronic lung allograft rejection, in lung transplant patients. However, the primary limitation lies in the lack of multi-institutional validation testing, emphasizing the need for further research validation.

## Kidney transplant

Kidney transplantation stands as the most frequently conducted solid organ transplant globally. Despite the increasing necessity for kidney transplantation, the pathology field struggles with a deteriorating workforce. According to the Organ Procurement and Transplantation Network (OPTN), as of July 23, 2023, there are 88,629 patients in the US awaiting a kidney transplant [ 78 ]. Similar to other organ transplant procedures, kidney transplantation and rejection assessment necessitate biopsies and pathologist evaluations. Integrating AI into this domain is a novel approach that holds impressive promise, facilitating the evaluation of kidney biopsy histopathology, guiding the treatment and management of transplant patients, and diagnosing rejection episodes. Several distinguished kidney biopsies scoring system models have emerged to fulfill these objectives, including the Remuzzi, Banff, Leuven, and Maryland Aggregate Pathology Index (MAPI), each with distinct scoring criteria tailored to the patient’s needs. The Remuzzi scoring system hinges on factors such as glomerulosclerosis, tubular atrophy, interstitial fibrosis, and arterial narrowing. In contrast, the Banff system focuses on vascular and other histologic abnormalities. The Leuven criteria closely resemble the Remuzzi model, incorporating elements such as Glomerulosclerosis, donor age, interstitial fibrosis, and tubular atrophy. On the other hand, the MAPI scoring system solely relies on histologic parameters, including glomerular sclerosis, arteriolar hyalinosis, cortical scar, and periglomerular fibrosis [ 79 – 83 ]. These scoring systems serve as invaluable tools in the comprehensive evaluation and management of kidney transplant patients, with AI revolutionize their implementation and enhance patient outcomes.

In 2023, Smith et al. [ 84 ] introduced a pioneering approach utilizing binary thresholding and a UNet model (a convolutional neural network that was developed for biomedical image segmentation [ 74 ]) for glomeruli segmentation in assessing interstitial inflammation from CD45-stained digital slides. Their study, encompassing 60 biopsies from 53 patients, uncovered a robust correlation between their automated inflammation scoring and the established Banff scoring system. Similarly, Hermsen et al. [ 85 ] Leveraged a UNet model to compute inflammatory and chronic characteristics in kidney transplant biopsies. They demonstrated a high correlation between the computed tissue features and Banff scoring by employing a structure segmentation CNN and a lymphocyte detection CNN on 125 whole-slide image pairs. In another breakthrough Zhengzi Yi et al. [ 86 ]. developed a DL model for characterizing pathological abnormalities in kidney transplant biopsies. Their model exhibited exceptional precision in spotting kidney tissue partitions and mononuclear leukocytes. Notably, the digital features revealed a significant correlation with revised Banff 2007 scores, offering heightened sensitivity to delicate pathological changes below the thresholds in the Banff scores.

Furthermore, Kers and colleagues conducted a study using 5,844 digital images of kidney transplant biopsies from 1,948 patients conducting a backdated, multicenter proof-of-concept. They tested different CNN models and showed that deep learning could help diagnose kidney transplant rejection [ 87 ]. These groundbreaking studies underscore the transformative potential of DL in revolutionizing the assessment and management of kidney transplant biopsies, ultimately enhancing patient care and outcomes.

## Liver transplant

Despite the persistent gap between organ supply and demand, more than a third of donor livers face rejection due to the perceived risk of early allograft dysfunction (EAD), as indicated by histopathological discoveries. The pivotal role of pathologist evaluation in assessing donor liver biopsies cannot be overstated, as it informs the decision to accept or reject probable donor livers. Moreover, liver fibrosis staging holds immense clinical significance in forecasting disease progression. Notably, the quantity and size of fibrotic portal tracts in liver biopsies are associated directly with the fibrosis stage, highlighting the critical importance of detailed portal tract region analysis in clinical practice. However, manual clarifications of portal tract regions pose significant challenges, including time constraints and substantial inter- and intra-observer variability. These limitations highlight the pressing need for more efficient and standardized approaches to portal tract analysis. Furthermore, donor livers undergo rigorous subjective pathologist review for steatosis assessment prior to transplantation, aiming to alleviate the risk of early allograft dysfunction [ 88 ].

The integration of advanced computational techniques, such as AI, holds promise in revolutionizing portal tract analysis, streamlining the evaluation process, and ultimately optimizing donor liver utilization and transplantation outcomes. In 2022, Narayan et al. [ 89 ] delved into the realm of AI for predicting donor liver allograft steatosis and early post-transplantation graft failure. Their groundbreaking research introduced a Computer Vision AI platform (CVAI) tailored to score donor liver steatosis. Comparative analysis against pathologist steatosis scores revealed that the CVAI steatosis EAD model exhibited slightly superior calibration. This finding warrants further exploration to determine which modality offers the most accurate and trustworthy prediction of post-transplantation outcomes.

Likewise, In 2022, Yu et al. [ 90 ]. Devised a revolutionary Multiple Up-sampling and Spatial attention-guided UNet model (MUSA-UNet) aimed at segmenting liver portal tract regions in liver whole-slide images (WSI), thereby correlating with liver fibrosis stage. Their innovative DL model, MUSA-UNet, demonstrated astonishing strictness in segmenting portal tract regions from liver tissue biopsy WSIs. It presents a hopeful potential to enhance liver disease diagnosis through computational means.

Furthermore, in 2022, Sun et al. [ 91 ] pioneered the development a DL convolutional neural network (CNN) capable of generating a steatosis probability map from hematoxylin and eosin-stained frozen section whole-slide images (WSIs). Subsequent calculations of percent steatosis revealed strong correspondence and agreement with interpretations in training and novel input test sets. Notably, these measurements exceeded estimates provided by pathologists at the time of initial evaluation. These advancements highlight the transformative potential of AI in improving liver disease diagnosis and predicting outcomes after transplantation.

## AI tools in transplant surgery

At some point, AI is expected to offer pioneering solutions that enhance surgical efficiency and improve patient outcomes in various surgical fields, including transplant surgery. By integrating AI into surgical practices, there is potential to revolutionize procedures, heralding a new era of personalized and data-driven healthcare. The field of surgery has been transformed by robotic assistance, which has enhanced surgical accuracy and patient outcomes across multiple specialties. The incorporation of cutting-edge technologies has bolstered the capabilities of surgical robots, resulting in improved patient care and fewer complications [ 92 , 93 ].

Compared to conventional surgical techniques, robotic systems offer numerous benefits. These include enhanced three-dimensional stereoscopic visualization, articulated instruments for improved dexterity, and software that eliminates tremors, thereby increasing surgical precision [ 92 ]. These attributes enable surgeons to execute intricate procedures with greater accuracy, particularly in gynecologic oncology and urologic operations [ 92 , 94 ]. The improved 3D perspective and magnified view of the surgical area contribute to a relatively blood-free operating environment and reduced surgeon exhaustion [ 94 ].

Artificial intelligence tools have been instrumental in advancing robotic-assisted surgery. Real-time analysis of surgical field images is facilitated by image recognition algorithms, while instrument movements are optimized by motion control systems [ 95 ]. Tasks such as suturing and tissue dissection have been enhanced through AI-driven automation, improving consistency and decreasing surgeon workload [ 95 ]. Surgical decision-making is supported by machine learning algorithms, which enhance the identification of minute and complex anatomical structures (94).

Notably, while robotic-assisted surgery generally improves precision, it may lead to extended operative durations compared to free-hand methods, especially in spine surgery. However, it typically results in reduced radiation exposure times, benefiting both patients and surgical teams [ 96 ].

AI algorithms analyze patient data to enhance organ compatibility during allocation. Robotic systems like the da Vinci Surgical System enable complex operations with fewer complications and quicker recovery times. The advent of robotic-assisted surgery has significantly altered how transplant procedures are conducted. Robots provide enhanced agility, immovability, and precision, allowing surgeons to perform complex tasks more accurately. Technologies such as the da Vinci Surgical System have been magnificently employed in kidney and liver transplants, reducing blood loss, minimizing surgical stress, and increasing the speed of patient recovery. By merging the precision of robotic equipment with the analytical power of AI algorithms, transplant surgeons have overpowered many challenges, paving the way for a more effective and easily reached era of organ transplantation [ 16 ]. The Hugo Robotic-Assisted Surgery (RAS) System by Medtronic represents another AI-integrated robotic technology with significant potential in the field of transplantation. Designed as a cost-effective platform for various surgical specialties, Hugo is still in the early stages of adoption for organ transplantation. However, its modular architecture and advanced imaging capabilities position it as a promising solution for kidney and liver transplants in the future. Key features of the Hugo system include its scalability and adaptability, allowing it to accommodate multi-surgeon teams seamlessly. Additionally, its cost-efficiency makes it particularly appealing for hospitals managing a high volume of transplant procedures. These advantages suggest that Hugo could play a pivotal role in advancing robotic-assisted surgeries within transplantation as the technology continues to evolve [ 97 ].

To summarize, robotic-assisted surgery, enhanced by AI tools, has transformed surgical practices by providing unprecedented levels of precision and control. As these technologies continue to develop, we can anticipate further improvements in surgical outcomes, fewer complications, and broader applications across various surgical specialties [ 93 , 95 ]. The integration of AI in robotic surgery shows great potential for advancing surgical care, although challenges such as high costs and the need for specialized training must be addressed for widespread adoption.

But at the final point, Integrating AI into the surgical process presents several challenges, including regulatory hurdles, the need for specialized surgeon training, and high implementation costs. Regulatory bodies must establish clear guidelines to ensure AI-driven surgical systems meet safety and ethical standards, which can delay adoption. Additionally, surgeons require extensive training to effectively use AI-assisted tools, as reliance on automated decision-making without proper oversight could lead to complications. The costs associated with acquiring and maintaining AI-powered surgical technologies, such as robotic systems and machine-learning software, can be prohibitive for many healthcare institutions. Balancing innovation with patient safety, legal compliance, and financial feasibility remains a key challenge in AI-driven surgery [ 98 ].

## AI tools in Post-transplant step



## Prediction of post-transplant outcome and mortality

Valuable decision-making and management in transplantation heavily hinge on perfectly predicting patient survival both on the waiting list and post-transplant. This is crucial for increasing efficacious transplant numbers and enhancing overall outcomes. To this end, several deep learning-based survival models have been developed. For instance, Luo et al. [ 99 ] utilized ML algorithms to create an analytical model identifying kidney transplant patients at higher risk of severe pneumonia during their post-transplant hospital stay. They analyzed recipient features using a variety of classifiers, including tree-based ensemble methods like Random Forest and AdaBoost, and non-ensemble classifiers such as support vector machines, Naïve Bayes, and logistic regression. The models’ effectiveness was assessed using the area under the precision-recall curve (AUPRC) and the area under the receiver operating characteristic curve (AUROC) through ten-fold cross-validation. The Random Forest model was beneficial for predicting severe pneumonia. It suggests that recipients with preoperative pulmonary infections, older patients, and those needing reoperation should be closely monitored to prevent severe pneumonia.

Several studies have utilized artificial neural networks (ANNs) for predicting graft outcomes following liver transplantation. ANNs have shown superior performance compared to traditional scoring systems, with one study reporting an area under the receiver operating characteristic curve (AUROC) of 0.82 for an ANN model versus 0.62 for the balance of risk (BAR) score and 0.57 for the Survival Outcome Following Liver Transplantation (SOFT) score [ 100 ]. Another ANN-based model was devised to forecast waitlist mortality post-transplant survival and simulate heart allocation processes. Trained on donor and recipient data, it accurately predicted waitlist and post-transplant mortality (AUROC = 89% and AUROC = 66%, respectively) [ 54 , 101 ]. Another ANN model achieved an AUROC of 0.84 compared to 0.68 for the donor risk index (DRI) and 0.64 for SOFT. Random forests have also been applied successfully, achieving AUROCs of 0.74, 0.68, and 0.64 for predicting 1-, 3-, and 5-year mortality respectively in pediatric heart transplantation [ 102 ]. Additionally, the performance of these models can be limited by data quality, small sample sizes, and the need for validation across different populations [ 103 ]. In pediatric heart transplantation, despite fair predictive utility, ML algorithms demonstrated poor sensitivity (0.07–0.49) in testing data, possibly due to missing data on key determinants of long-term survival [ 102 ].

Delayed graft function (DGF), a frequent early post-transplant complication, is predictive of adverse outcomes like hospital readmission, diminished long-term graft function, and lower graft and patient survival rates [ 104 ]. DGF impacts immediate and long-term results, making the prediction of graft failure before kidney transplantation vital for managing chronic kidney disease patients. Despite the existence of several ML-based prediction models for kidney graft outcomes, many rely on small datasets and do not incorporate time-to-event (survival) data, focusing instead on binary outcomes (failure or not).

Irish et al. [ 105 ] introduced a model to predict DGF following renal transplantation. This model was developed using multivariable logistic regression analysis on data from 24,337 deceased donor renal transplant recipients between 2003 and 2006. They created a nomogram to illustrate the relative contributions of risk factors and developed a web-based calculator ( http://www.transplantcalculator.com/DGF ) for easy access. With a c-statistic of 0.704, this model was the first to use patient characteristics for DGF prediction and proved generalizable to external populations [ 106 ].

Senanayake et al. [ 107 ] developed two distinct ML-based models to predict graft failure in live and deceased donor kidney transplants, utilizing time-to-event data from a large national dataset from Australia and New Zealand Dialysis and Transplant Registry. This dataset encompassed 3,758 live donor transplants and 7,365 deceased donor transplants performed between January 1, 2007, and December 31, 2017. They employed three ML methods (survival tree, random survival forest, and survival support vector machine) alongside the traditional Cox proportional regression method to develop predictive models for both transplant types.

Kawakita et al. [ 108 ] compared various ML algorithms (logistic regression, elastic net, random forest, artificial neural network, and extreme gradient boosting) to evaluate their effectiveness in predicting DGF in kidney transplant recipients. Their models, which incorporated 30 variables (13 donor-related, eight recipient-related, and five transplant-related), demonstrated improved discrimination compared to standard regression models, as evidenced by higher areas under the receiver operating characteristic curves.

Agasthi et al. [ 109 ] used ML to develop a risk prediction model for survival and graft failure (GF) five years post-orthotopic heart transplant (OHT). Analyzing data from the International Society of Heart and Lung Transplant (ISHLT) registry, they examined 15,236 patients who underwent OHT from January 2005 to December 2009. Using 342 variables, they developed a gradient-boosted machine (GBM) model to predict GF and mortality five years after hospital discharge. They found that length of hospital stays, recipient and donor age, body mass index, and ischemic time were the most influential factors in predicting five-year mortality and graft failure.

The findings from these studies suggest that AI and ML models generally outperform traditional regression analyses in predicting graft failure and mortality. However, while ML models show strong prognostic power for one-year outcomes, their accuracy diminishes for longer-term forecasts. These models are limited by the data available to them, demonstrating a need for more comprehensive data to better calculate long-term outcomes in post-transplant patients. Fascinatingly, the predictive variables differ based on the time horizon, with different variables being noteworthy for one-year versus five-year mortality predictions [ 110 , 111 ].

## Rejection risk evaluation

Numerous studies have underscored the profound influence of acute rejection (AR) on long-term graft survival, establishing it as a pivotal factor in chronic graft dysfunction and failure. To tackle these challenges, the FDA convened public workshops in June 2010 and April 2017, focusing on antibody-mediated rejection (AMR) treatment and the design of clinical trials aimed at enhancing long-term outcomes. With AI advancements, these trials have become more feasible [ 112 , 113 ]. While various studies have attempted to predict rejection and identify high-risk patients, their accuracy has often been limited due to the complexity of contributing factors. ML offers a solution by managing extensive data points and integrating clinical, genetic, metabolomic, and pathology-based variables for improved predictive capability. ANNs have consistently demonstrated superior accuracy in predicting graft rejection for renal and liver transplants compared to traditional models [ 114 – 116 ].

Shaikhina et al. [ 117 ] applied ML algorithms to a small dataset of 80 kidney transplants, considering factors like age, gender, HLA Class I and II, years on dialysis, IgG subclass levels, highest IgG DSA level, and number of previous transplants. Their model achieved an 85% accuracy in predicting acute antibody-mediated rejection (AMR) 30 days post-transplant.

Tapak et al. [ 116 ] identified extrapolative elements coupled with kidney transplant rejection using the ANN approach and compared the results with those acquired by logistic regression (LR). They used information regarding 378 patients who had undergone kidney transplantation from a reflective study conducted in Hamadan, Western Iran, from 1994 to 2011. ANN was used to pinpoint potential essential risk factors for chronic nonreversible graft rejection. They point out that the ANN model overtook LR in predicting kidney transplantation failure.

In 2016, Esteban et al. [ 118 ] utilized a recurrent neural network (RNN) for foretelling acute rejection in kidney transplant using Prescribed medication (Cyclosporin, Furosemide…) Creatinine (high/normal/low) Leukocytes (high/normal/low) as predictive elements. RNN combines static and dynamic information to predict upcoming events. They worked with a database collected in the Charité Hospital in Berlin that contains complete information concerning patients who experienced kidney transplantation.

Suthanthiran et al. [ 119 ] collected 4300 urine specimens from 485 kidney-graft recipients from day 3 through month 12 after transplantation. Messenger RNA (mRNA) levels were quantified in urinary cells and connected with allograft-rejection status using logistic regression with the knowledge that the standard test for diagnosing acute rejection in kidney transplants is renal biopsy. The authors obtained an AUC of 0.85 with a three-gene expression signature for the perception between acute rejection and no rejection in their cohort and an AUC of 0.74 upon external validation.

Zare et al. [ 115 ] achieved a predictive model based on ANN technique and figure out the best time for early predicting acute allograft rejection after transplantation in liver transplant recipients. Feed-forward, back-propagation neural network was developed to predict acute rejection in liver transplant recipients using clinical and biochemical data from 148 liver transplant recipients over days 3, 7, and 14 post-transplantations. Their study suggests that ANN could be a valuable addition to conventional liver function tests for monitoring liver transplant recipients in the early postoperative period.

Abdeltawab et al. [ 120 ] combined diffusion-weighted MRI data with clinical biomarkers such as creatinine clearance and serum plasma creatinine. Their convolutional neural network (CNN) accurately identified 92.9% of rejected kidney grafts, regardless of scanner type and image collection protocol variations. Reeve et al. [ 121 , 122 ] employed clustering analysis to classify phenotypes linked to kidney transplant rejection. Training an algorithm on 1,208 kidney biopsies, they aimed to replace binary histologic diagnoses with a probabilistic model, identifying subtypes of T cell-mediated rejection (TCMR) and antibody-mediated rejection (AMR).

## Post-transplant complications

Clinical outcomes following transplantation are significantly impacted by the prompt identification of complications in the immediate postoperative period. Post-transplant management protocols can vary based on factors such as the institution, surgeon, and individual patient characteristics. While cardiac, lung, and liver transplant recipients typically require admission to a surgical intensive care unit after their procedure, the length of stay in the unit may not necessarily correlate with the severity of the patient’s condition. Early identification of potential decompensation on surgical wards can help prevent failure-to-rescue scenarios by promptly transferring patients back to intensive care units. Although the literature does not offer a definitive explanation for the strong association between length of hospital stay and graft patency and mortality, it is well-established that an increase in complications such as bleeding or infections, as well as the severity of these complications, can lead to a prolonged hospital stay [ 123 , 124 ]. Post-transplant complications are often detected through biopsy, but the integration of ML into imaging techniques may eliminate the need for invasive procedures and reduce the associated risks of hemorrhage, infection, and damage to nearby anatomical structures. Diffusion-weighted MRI images have been incorporated into models of non-imaging inputs, such as creatine clearance and serum plasma creatine, to identify early graft dysfunction with over 90% sensitivity and specificity. Prior to liver transplantation, the degree of graft steatosis is a crucial factor for surgeons in predicting post-transplant graft function. Given the high cost and inefficiency of biopsy and histological methods for assessing steatosis, physicians rely on donor clinical characteristics and visual evaluations of the graft. However, the subjective and variable nature of this process poses a challenge, prompting researchers to utilize ML tools on photographic data for detecting hepatic steatosis.

Conversely, acute kidney injury (AKI) is a common complication following liver transplantation (LT). It serves as an indicator of poor prognosis. Predicting the risk of this complication before or after transplantation can be instrumental in preventing adverse outcomes. As previously discussed, delayed graft function (DGF) in kidney transplantation refers to the need for dialysis within the first-week post-transplantation and is linked to a higher risk of graft loss, prolonged hospitalization, and increased costs. ML algorithms have the potential to identify valuable prognostic markers. In the case of lung transplant recipients, chronic lung allograft dysfunction (CLAD) affects over 50% within the initial 5 years post-transplantation, significantly impacting long-term survival. Bronchiolitis obliterans syndrome (BOS) represents an obstructive form of CLAD resulting from chronic immune-mediated rejection, leading to reduced airflow indicated by decreased forced expiratory volume in the first second (FEV1).

## Kidney transplant

Williams et al. [ 125 ] employed The RF methodology to detect urine proteins that have predictive value for delayed graft function (DGF) using a targeted urine proteome assay. They collected urine samples from 52 patients with intermediate, slow, and delayed graft function within 12–18 h post-surgery. Their analysis identified four key urine proteins that exhibited changes in recipients with DGF. The sensitivity of these proteins in predicting DGF was 77.4%, with a specificity of 82.6%.

Daher Costa et al. [ 126 ] conducted a retrospective analysis of 443 kidney transplants from brain-dead deceased donors in two Brazilian centers. Using predictive modeling, they evaluated various donor maintenance variables, including arterial blood gas pH, serum sodium, blood glucose, urine output, mean arterial pressure, vasopressor use, and reversed cardiac arrest. Their study highlighted the significance of donor maintenance variables such as urine output and mean arterial pressure, which were not considered in other regression-based risk scores for predicting post-transplant DGF. However, it is important to note that the retrospective nature of the study may limit the timeliness of the data and its ability to represent the overall picture of donor kidney maintenance fully.

Villeneuve et al. [ 127 ] utilized The RF approach to assess the health-related quality of life (HRQOL) and its determinants in 337 kidney transplant recipients during the first three years post-transplantation. Their analysis revealed a significant association between HRQOL one month after transplantation and HRQOL three and 36 months after transplantation. Unlike conventional models, the ensemble of ML algorithms employed in this study allowed for analyzing both quantitative and qualitative variables without limitations on the covariates tested.

## Liver transplant

Moccia et al. [ 128 ] utilized a semi-supervised classification technique to assess graft steatosis in liver transplantation. They employed 40 liver images taken by a smartphone camera in the operating room, resulting in 600 liver patches. Alongside clinical variables and blood sample tests, they used an SVM model to evaluate the steatosis of the grafts qualitatively. The model achieved an accuracy of 0.88, sensitivity of 0.95, and specificity of 0.81, with liver biopsy as the reference method.

Bhat et al. [ 129 ] conducted a study to determine significant predictors and survival outcomes of new-onset diabetes after transplant (NODAT) in liver transplant (LT) recipients. They analyzed data from the Scientific Registry of Transplant Recipients, including all adult liver transplantation recipients between October 1, 1987, and March 31, 2016. Various ML methods were utilized, and the data were split into training and validation sets. The study found that older, male, and obese recipients are at a higher risk of developing NODAT. Donor characteristics, however, do not impact the risk. Additionally, using sirolimus-based immunosuppression was linked to a significantly increased risk of NODAT compared to other immunosuppressants.

Tanaka and Voigt [ 130 ] employed a decision tree approach to categorize liver transplant (LT) recipients based on their risk of developing non-melanoma skin cancers (NMSCs). NMSCs are the most common de novo malignancies in LT recipients. They are known for their aggressive behavior, leading to higher mortality rates. The researchers conducted Cox regression analysis to identify the predictive variables for inclusion in the decision tree analysis. The data used in the study were sourced from the Organ Procurement Transplant Network (OPTN) STAR files of September 2016, comprising a total of 102,984 cases. The decision tree model effectively stratified the long-term risk of developing NMSCs post-LT.

Lee et al. [ 131 ] devised a new scoring system utilizing the LASSO regression method to forecast the likelihood of persistent alcohol consumption following a liver transplant in patients with alcohol hepatitis. The aim was to prioritize individuals with a lower risk of relapse for early transplantation. The study involved 134 liver transplant recipients, and the resulting model achieved a C-statistic of 0.76 after internal cross-validation. Four key pre-transplant variables were identified as significant predictive risk factors for sustained alcohol consumption post-transplant: consuming more than ten drinks per day upon initial hospitalization, a history of illicit substance abuse, a history of alcohol-related legal issues, and a history of multiple rehabilitation attempts.

Lee et al. [ 132 ] conducted a study to compare the effectiveness of ML approaches and logistic regression analysis in predicting AKI after liver transplantation. The researchers examined a total of 1211 patients and collected data on preoperative and intraoperative anesthesia and surgery-related variables. The primary outcome of interest was the occurrence of postoperative AKI, as defined by the acute kidney injury network criteria. The researchers employed various ML techniques to assess the predictive performance of different methods, including decision tree, random forest, gradient boosting machine, support vector machine, naïve Bayes, multilayer perceptron, and deep belief networks. These techniques were then compared to logistic regression analysis based on the area under the receiver-operating characteristic curve (AUROC). Among the patients included in the study, 365 individuals (30.1%) developed AKI. The results indicated that the gradient boosting machine exhibited the highest performance in terms of AUROC among all the analyzed techniques for predicting AKI at all stages. Therefore, the gradient boosting machine demonstrated superior predictive ability with the highest AUROC value.

He et al. [ 133 ] utilized 493 recipients (donations after cardiac death LT) to study AKI, which was defined based on the clinical practice guidelines of kidney disease: improving global outcomes (KDIGO). The clinical data of patients with AKI and those without AKI were compared. In addition to logistic regression analysis, four predictive ML models were developed using various algorithms: random forest, support vector machine, classical decision tree, and conditional inference tree. The random forest model, which was based on ML algorithms for predicting AKI after DCDLT, exhibited superior predictive power compared to the other models in the study. This indicates that ML techniques could serve as effective tools for forecasting AKI following DCDLT.

Chen et al. [ 134 ] conducted a study on developing and evaluating novel ML models to predict pneumonia after liver transplantation. They retrospectively extracted data from electronic medical records of 786 adult patients who underwent liver transplantation at the Third Affiliated Hospital of Sun Yat-sen University between January 2015 and September 2019. The data was randomly divided into a training set and a testing set, with a total of 591 LT patients included in the analysis. Among them, 253 patients (42.81%) were diagnosed with postoperative pneumonia, which was linked to increased postoperative hospitalization and mortality. The study revealed that the XGBoost model, utilizing 14 common variables, could effectively predict postoperative pneumonia in LT patients.

Nam et al. [ 135 ] developed a novel model to forecast the recurrence of hepatocellular carcinoma (HCC) after liver transplantation (LT). Their study involved 563 patients undergoing LT for HCC at three prominent LT centers in Korea. The researchers derived a MoRAL-AI (Model for Recurrence of HCC after Liver Transplantation using AI from the derivation cohort, utilizing a deep neural network based on residual blocks. Among the parameters considered in the MoRAL-AI, tumor diameter exhibited the highest weighted significance, followed by alpha-fetoprotein levels, age, and protein induced by vitamin K absence-II. Notably, the MoRAL-AI demonstrated superior predictive capability for tumor recurrence after LT compared to conventional models.

Similarly, Ivanics et al. [ 136 ] developed a calculator to assess the risk of hepatocellular carcinoma recurrence following liver transplantation. The researchers identified patients with HCC who were listed for LT between 2000 and 2016, with a subset of 739 patients undergoing LT utilized for modeling purposes. The dataset encompassed serial imaging, alpha-fetoprotein (AFP) levels, locoregional therapies, treatment response, and post-transplantation outcomes. The mean cross-validated concordance index was employed to evaluate the performance of various ML algorithms, including CoxNet (regularized Cox regression), survival random forest, survival support vector machine, and DeepSurv. The selected CoxNet model was subsequently validated by comparing it with other existing recurrence risk algorithms using a separate test set and the Hazard Associated with Liver Transplantation for Hepatocellular Carcinoma.

Jain et al. [ 137 ] conducted a study where they developed ML Models to forecast Major Adverse Cardiovascular Events (MACE) following Orthotopic Liver Transplantation (OLT). The study encompassed a comprehensive dataset of 1,459 consecutive patients who underwent LT between January 2008 and December 2019. To construct their models, the researchers employed a Retrospective cohort study design. Logistic regression, least absolute shrinkage and selection surgery regression, random forests, support vector machine, and gradient-boosted modeling (GBM) were utilized to model all-cause mortality and cardiovascular mortality. The data was divided into training and testing cohorts to build the models, and their performance was evaluated using five-fold cross-validation based on the area under the receiver operating characteristic curve and Harrell’s C statistic.

In a separate study, Nitski et al. [ 138 ] investigated the effectiveness of deep-learning algorithms in predicting complications leading to mortality after liver transplantation across different time points. The study involved a cohort of 3,269 patients, and 63 input variables were recorded in the Canadian University Health Network (UHN) database. Both databases exhibited minimal missing values, ensuring the reliability of their predictive capabilities. The ANNs models generated impressive AUCs ranging from 0.847 to 0.871 for predicting death due to graft failure within the first-year post-transplantation. These results were successfully replicated in the testing group, further validating the accuracy of the models.

## Lung transplant

In their study, Barbosa Jr et al. [ 139 ] utilized a Support Vector Machine (SVM) to examine quantitative CT imaging data from 71 patients as a means of early detection for Chronic Lung Allograft Dysfunction (CLAD). By analyzing the quantitative CT scans taken during the initial post-transplant visit, the SVM algorithm successfully identified individuals at a higher risk of developing Bronchiolitis Obliterans Syndrome (BOS) soon. Interestingly, pulmonary function tests did not exhibit any significant changes during the early stages of the disease. The model achieved an impressive accuracy rate of 85% by utilizing three specific features extracted from the patient images.

In a separate study conducted by Sadat Hosseini-Baharanchi et al. [ 140 ], a Bayesian Competing Risks Analysis was employed to evaluate the occurrence of BOS-related deaths in Iranian Lung Transplant Recipients. This retrospective cohort study included 44 recipients of lung transplants who survived for a minimum of three months post-transplantation at the Masih Daneshvari Hospital in Tehran, Iran, between 2000 and 2014. The primary outcome of interest was the time interval between lung transplantation and the development of BOS and/or death (excluding deaths caused by BOS). The researchers utilized competing risk analysis to assess the impact of various factors on the cumulative incidence function of BOS and death. Employing a Fine and Gray model with a Bayesian approach, the study found that CMV infection was a predictor for an increased risk of BOS in the studied patients. Additionally, bilateral transplantation and CMV infection were identified as significant predictors of mortality within the sample population.

## Optimizing the immunosuppression regimens

Anti-rejection immunosuppression medications are commonly administered following a transplant procedure. Yet, a majority of databases do not gather information regarding patients’ specific immunosuppression regimens. Research has indicated that this particular factor significantly predicts graft failure, surpassing its impact on mortality prediction. This could be attributed to the fact that these regimens reduce the risk of host rejection while potentially causing harmful side effects on organs such as the kidneys [ 141 ]. The selection of immunosuppression regimens post kidney and liver transplants exhibits considerable variability. Before establishing national registries, analyses have revealed that specific protocols at different medical centers primarily influence this diversity.

Nevertheless, studies have demonstrated that both the immunosuppression regimens and individual patient characteristics can significantly affect the 3-year survival of grafts and the complications arising from immunosuppression when comparing similar patients on different regimens. Therefore, the development of a personalized approach to immunosuppression could potentially decrease post-transplant morbidity. For instance, regimens involving early withdrawal of steroids following a kidney transplant have been associated with fewer complications in elderly populations [ 142 ]. Although overall toxicity may increase mortality rates, it does not have as significant an impact on graft failure. It is worth noting that numerous databases have not collected data on patients’ immunosuppression regimens, possibly due to a perceived lack of importance.

Consequently, ML models could facilitate broader data collection to enhance the prognostic accuracy of these models. Including patients’ immunosuppression regimens could improve prognostic accuracy and potentially assist in optimizing individualized regimens for each patient. Instances of graft rejection have been linked to suboptimal levels of immunosuppressive drugs, and given the various pharmacodynamic and pharmacokinetic factors involved, the utilization of ML algorithms on extensive datasets could enable a comprehensive analysis of these factors and potentially identify the most suitable regimen for each patient [ 143 ].

One of the primary difficulties in the management of solid organ transplants involves the identification of the most effective dosage for immunosuppression and other medications that are sensitive to dosage. This challenge becomes particularly complex when dealing with patients who have renal or hepatic dysfunction. Waiting for drug levels to stabilize can lead to not only delayed discharge, but also the failure of the transplanted organ and the occurrence of adverse drug reactions. As personalized medicine gains prominence, the unique characteristics of individual patients become crucial in understanding how drugs are processed in their bodies. This suggests that incorporating pharmacokinetic data into machine-learning algorithms could enhance the accuracy of clinical dosing decisions.

## Tacrolimus

In their study Thishya et al. [ 21 ] investigated the impact of genetic polymorphisms in ABCB1 and CYP3A5 on the bioavailability of tacrolimus and the risk of post-transplant diabetes. They employed artificial neural network (ANN) and logistic regression (LR) models to predict the bioavailability of tacrolimus and the risk of post-transplant diabetes respectively. Additionally, the researchers developed a three-level artificial neural network to identify the variables that influenced both tacrolimus bioavailability and the risk of diabetes in renal transplant recipients. In a separate study by Tang et al. [ 19 ] the authors compared linear regression with eight different ML models to forecast stable tacrolimus dosing in renal transplant recipients. The results indicated that regression trees exhibited the best overall performance. Although these trees outperformed linear regression, their predictive ability was comparable to the other seven ML techniques.

Furthermore, Zarrinpar et al. [ 144 ] devised a personalized dosing model for tacrolimus in liver transplant recipients. This model was based on a second-order polynomial equation, resulting in consistent and appropriate adjustments to tacrolimus dosing.

Storset et al. [ 145 ] examined the application of computerized tacrolimus dosing in de novo renal transplant recipients. They compared the efficacy of computerized dosing with traditional dosing methods employed by experienced transplant physicians. The findings revealed that computerized dose individualization significantly enhanced the proportion of tacrolimus concentrations within the desired range, in contrast to conventional dosing. Moreover, high-risk patients experienced a shorter time to achieve target levels with computerized dosing. Additionally, computerized dosing exhibited advantages in terms of glucose metabolism and renal function.

McMichael et al. [ 146 ] conducted an evaluation of an innovative dosing system known as the “intelligent” dosing system (IDS), which aimed to optimize FK 506 and prednisone. The IDS utilized stochastic open-loop control theory to optimize drug dosing and demonstrated accurate prediction of FK 506 plasma levels. The model achieved a remarkable 95% accuracy predicting in describing the correlation between FK 506 dosage and plasma level. Furthermore, the study found no biases in the dosing predictions made by the model, confirming its reliability in providing precise estimations. Importantly, the study emphasized the unbiased nature of the dosing predictions, further validating the model’s accuracy.

Seeling et al. [ 147 ] devised a knowledge-based system to guide tacrolimus therapy for kidney transplant recipients. The primary aim of the research was to establish adaptation guidelines for tacrolimus treatment by analyzing a clinical dataset and incorporated them into a clinical decision-support tool. The study drew upon patient information spanning from 1995 to 2008, sourced from the Department of Nephrology and Dialysis at the Vienna General Hospital. This dataset encompassed patient characteristics, laboratory results, duration post-kidney transplantation, and other immunosuppressive medications administered. The investigators employed a regression tree methodology to segment the data into homogeneous clusters and devised semi-automated models for these clusters to predict the drug concentration for the subsequent ward round.

## Cyclosporine

Camps-Valls et al. [ 148 ] explored the application of neural networks in personalizing the dosage of cyclosporine A (CyA) for kidney transplant patients. They utilized various types of neural networks, such as multilayer perceptron (MLP), finite impulse response (FIR), and Elman recurrent networks. To create a comprehensive model, the researchers devised a two-model approach where the first model predicted the blood concentration, which then served as input for the dosage prediction model. The training of these models involved data from 22 patients, while testing was conducted using data from 10 patients. Notably, the ensemble of FIR and Elman networks exhibited the highest performance among the different neural network configurations.

In another related study, Goren et al. [ 149 ] examined the adaptive-network-based fuzzy inference system (ANFIS) for predicting cyclosporine blood levels in renal transplantation patients. The ANFIS model was developed based on therapeutic drug monitoring (TDM) data collected from 138 patients. The model incorporated 20 input parameters, including concurrent drug usage, blood levels, sampling time, age, gender, and dosing intervals. The authors reported that the ANFIS model demonstrated accurate prediction capabilities for cyclosporine concentration in blood samples.

## Mycophenolic acid (MPA)

The estimation of the area under the curve (AUC) of MPA is crucial for optimizing treatment and improving patient outcomes in therapeutic drug monitoring for MPA. In a study conducted by Woillard et al. [ 150 ], a machine-learning model was developed to accurately estimate the concentration of MMF in transplant patients. Specifically, the models focused on estimating the concentration of MMF in patients who had undergone kidney or heart transplants.

The researchers utilized extreme gradient boosting (Xgboost R package) ML models to develop these models. The models were trained using a dataset of 12,877 MPA AUC requests from 6,884 patients. These requests were sent to the Immunosuppressant Bayesian Dose Adjustment expert system, which provided AUC estimation and dose recommendations based on MPA concentrations measured at three sampling times (approximately 20 min, 1 h, and 3 h after dosing).

The dataset was divided into a training set (75%) and a test set (25%). The Xgboost models in the training set were evaluated using the root mean squared error (RMSE) in a 10-fold cross-validation experiment. The model with the lowest RMSE was then evaluated in the test set and in four independent full-pharmacokinetic datasets from renal or heart transplant recipients. The models took into account various factors, including two or three concentrations, differences between these concentrations, relative deviations from theoretical sampling times, the presence of a delayed absorption peak, and five covariates (dose, type of transplantation, associated immunosuppressant, age, and time between transplantation and sampling). The results of the study demonstrated that the developed model accurately estimated the AUC of MPA over a 12-hour period. These models have the potential to be utilized in routine exposure estimation and dose adjustment of MPA, providing valuable guidance for clinicians in optimizing treatment for transplant patients. In Table 3 the information of significant studies focusing on the application of different AI models in organ transplant are summarized.

Table 3 Summary of studies using AI for organ transplant Author, Year Purpose AI-model dataset Giuste et al., 2023 [ 72 ] Enhancing risk assessment of rare pediatric heart transplant rejection through the generation of synthetic images Progressive and Inspirational GAN 12 non rejection and 12 rejection slides Lipkova et al., 2022 [ 74 ] Assessment of cardiac allograft rejection from endomyocardial biopsies Pre-trained deep residual CNN Training: 1352 WSI slides; Validation: 1840 WSI slides Seraphin et al., 2023 [ 73 ] Evaluated the feasibility of using deep learning in predicting the degree of cellular rejection from pathology slides and predict heart transplant rejection from routine pathology slides attention-based deep neural network 1079 histopathology slides from 325 patients from three transplant centres in Germany Hsich et al., 2019 [ 53 ] Variables of importance in the Scientific Registry of Transplant Recipients database predictive of heart transplant waitlist mortality RF model 33,069 waitlist Medved et al., 2018 [ 54 ] Simulating the outcome of heart allocation policies using deep neural networks Neural network-based algorithm named Lund Deep Learning Transplant Algorithm (LuDeLTA) 30,584 recipients and 18,982 donors Ayllón et al., 2018 [ 55 ] Validation of artificial neural networks as a methodology for donor–recipient matching for liver transplantation ANN mode 822 donor–recipient pairs Moreno et al., 2017 [ 56 ] Solving an imbalanced liver transplantation problem Ordinal neural network 634 donor–recipient pairs Placona et al., 2020 [ 57 ] Predict the delay or discard of adult deceased donors based on donor-free-text data Natural Language Processing mode 74,041 donors Bricen˜o et al., 2014 [ 64 ] Use of artificial intelligence as an innovative donor-recipient matching model for liver transplantation Multiple regression models, ANN, Neural Net Evolutionary Programming utilized 64 donor and recipient variables from 1003 liver transplants across 11 Spanish centers Cruz-Ramírez et al., 2013 [ 17 ] Predicting patient survival after liver transplantation Evolutionary multi-objective artificial neural networks dataset of liver transplants collected by eleven Spanish hospitals Furness et al., 1999 [ 67 ] Assist in the histological diagnosis of early acute renal allograft rejection Neural network transplant biopsies from Oxford Tong et al., 2017 [ 68 ] Predicting heart rejection Deep neural network histopathological whole-slide imaging of 43 patient Hermsen et al., 2019 [ 69 ] Histopathologic Assessment of Kidney Tissue Convolutional neural network 40 whole-slide images of stained kidney transplant biopsies Perez-Sanz et al., 2021 [ 70 ] Efficiency of machine learning algorithms for the determination of macrovesicular steatosis in frozen sections stained with Sudan to evaluate the quality of the graft in liver transplantation Machine learning and artificial vision 20 donor liver samples Gholamzadeh et al., 2022 [ 77 ] Improve lung transplantation outcomes and complications Support vector machine, logistic regression, Random Forests, Bayesian network, Markov Model, KNN, k- means, Gradient Boosting trees (XGBoost), Convolutional Neural Network electronic databases from January 2000 to June 2022 Davis et al, 2020 [ 78 ] Detection of ACR in lung transplant biopsies AI model 3349 annotations Smith et al., 2023 [ 85 ] Quantifying the amount of non-glomerular inflammation within the cortex A UNet model 60 biopsies from 53 patients Hermsen et al., 2022 [ 86 ] Quantifying the chronic and inflammatory lesions in kidney transplant biopsies A UNet model 125 WSI pairs of periodic acid-schiffand CD3-stained slides Kers et al., 2022 [ 88 ] Classifying histology of kidney allograft biopsies Single and serial CNN 5844 WSIs from 1948 patients Zhengzi Yi et al., 2022 [ 87 ] Characterizing pathological abnormalities in kidney transplant biopsies DL model slide images from 789 transplant biopsies Narayan et al., 2022 [ 90 ] Prediction of donor liver allograft steatosis and early post-transplantation graft failure CVAI model consisting of Fully Convolutional Networks (FCN) and UNet 25,494 images from 90 liver biopsies Yu et al., 2022 [ 91 ] Segmentation of portal tract regions from whole-slide images of liver tissue biopsies MUSA-UNet, FCN, UNet, DeepLab V3 53 WSIs (Training and Validation: 30; Testing: 20) Sun et al., 2022 [ 92 ] Quantify percent steatosis in donor liver biopsy frozen sections Pre-trained VGG16 (Truncated at bottleneck layer) 96 WSIs (Training: 30; Testing: 66) Luo et al., 2020 [ 100 ] Identifying kidney transplant patients at higher risk of severe pneumonia during their post-transplant hospital stay Random Forest, AdaBoost, support vector machines, Naïve Bayes, and logistic regression five hundred nineteen patients who underwent transplantation from January 2015 to December 2018 were included Irish et al., 2010 [ 106 ] Predicting DGF following renal transplantation Multivariable logistic regression analysis 24,337 deceased donor renal transplant recipients between 2003 and 2006 Senanayake et al., 2019 [ 108 ] Predicting graft failure in live and deceased donor kidney transplants Survival tree, random survival forest, and survival support vector machine, Cox proportional regression large national dataset from Australia and New Zealand Dialysis and Transplant Registry Kawakita et al., 2020 [ 109 ] Evaluate various machine learning effectiveness in predicting DGF in kidney transplant recipients Logistic regression, elastic net, random forest, artificial neural network, and extreme gradient boosting data from adult DDKT recipients for model development ( n = 55,044) and validation ( n = 6176). Agasthi et al., 2020 [ 110 ] A risk prediction for survival and graft failure (GF) five years post-orthotopic heart transplant (OHT) Gradient-boosted machine (GBM) model 15,236 patients from January 2005 to December 2009 Shaikhina et al., 2019 [ 118 ] Predicting acute antibody-mediated rejection (AMR) 30 days post-transplant Decision tree and random forest 80 kidney transplants Tapak et al., 2017 [ 117 ] Identify potential essential risk factors for chronic nonreversible graft rejection Logistic regression, ANN 378 patients from Hamadan, Western Iran, from 1994 to 2011 Esteban et al., 2016 [ 119 ] Predicting acute rejection in kidney transplant Recurrent neural network (RNN) database collected in the Charité Hospital in Berlin Suthanthiran et al., 2013 [ 120 ] Prediction of Urinary-cell mRNA profile and acute cellular rejection in kidney allografts Logistic regression 4300 urine specimens from 485 kidney-graft recipients from day 3 through month 12 after transplantation Zare et al., 2017 [ 116 ] Early predicting acute allograft rejection after transplantation in liver transplant recipients ANN data from 148 liver transplant recipients over days 3, 7, and 14 post-transplantations Abdeltawab et al., 2019 [ 121 ] Early Assessment of Transplanted Kidney Dysfunction Convolutional neural network scans collected from 56 subjects from geographically diverse populations Reeve et al., 2017, 2019 [ 122 , 123 ] Assessing rejection-related disease in kidney transplant biopsies with the aid of automated kidney transplant biopsy reports Clustering analysis 1,208 kidney biopsies Williams et al., 2017 [ 126 ] Identify protein biomarkers of delayed recovery from kidney transplant. Random Forest analysis 42 patients Daher Costa et al., 2020 [ 127 ] Evaluated the risk factors for delayed graft function (DGF) Retrospective analysis 443 kidney transplants from brain-dead deceased donors in two Brazilian centers Villeneuve et al., 2016 [ 128 ] Assess the health-related quality of life after kidney transplant Random forest 337 kidney transplant recipients during the first three years post-transplantation Moccia et al., 2018 [ 129 ] Assessment of graft steatosis in liver transplantation Semi-supervised classification technique 40 liver images Bhat et al., 2018 [ 130 ] Identifying key predictors and survival outcomes of new-onset diabetes after transplant in liver transplant recipients Various machine learning approaches data from the Scientific Registry of Transplant Recipients, between October 1, 1987, and March 31, 2016 Tanaka and Voigt, 2018 [ 131 ] Risk Evaluation of developing non-melanoma skin cancers in liver transplant recipients Decision tree 102,984 cases Lee et al., 2019 [ 132 ] Prediction the risk of persistent alcohol consumption following a liver transplant in patients with alcohol hepatitis The LASSO regression method 134 liver transplant recipients Lee et al., 2018 [ 133 ] Evaluation of the effectiveness of ML approaches and logistic regression analysis in predicting AKI after liver transplantation Decision tree, random forest, gradient boosting machine, support vector machine, naïve Bayes, multilayer perceptron, and deep belief networks 1211 patients He et al., 2021 [ 134 ] Predicting acute kidney injury following donation after cardiac death liver transplantation Random forest, support vector machine, classical decision tree, and conditional inference tree 493 recipients Chen et al., 2021 [ 135 ] Predict pneumonia after liver transplantation Many novel ML models 786 adult patients Nam et al., 2020 [ 136 ] Forecast the recurrence of hepatocellular carcinoma after liver transplantation Deep neural network (MoRAL-AI) 563 patients Ivanics et al., 2022 [ 137 ] Assessing the risk of hepatocellular carcinoma recurrence following liver transplantation CoxNet (regularized Cox regression), survival random forest, survival support vector machine, and DeepSurv 739 patients Jain et al., 2021 [ 138 ] Forecasting Major Adverse Cardiovascular Events following Orthotopic Liver Transplantation Retrospective cohort study design 1,459 consecutive patients Nitski et al., 2021 [ 139 ] Predicting complications leading to mortality after liver transplantation ANN 3,269 patients Barbosa Jr et al., 2018 [ 140 ] Assessing early detection for Chronic Lung Allograft Dysfunction Support Vector Machine 71 patients Sadat Hosseini-Baharanchi et al., 2016 [ 141 ] Evaluate the occurrence of Bronchiolitis Obliterans Syndrome-related deaths in Lung Transplant Recipients Retrospective cohort study 44 recipients Thishya et al., 2018 [ 21 ] predicting the bioavailability of tacrolimus in patients with renal transplantation Artificial neural network (ANN) and logistic regression (LR) models blood samples were collected from 54 patients on day 1, 3, 5, 10, 15, 30, 45, 60 and 90 day post-transplantation Tang et al., 2017 [ 19 ] Predicting Tacrolimus Stable Dose in Renal Transplant Recipients Multiple linear regression (MLR), artificial neural network (ANN), regression tree (RT), multivariate adaptive regression splines (MARS), boosted regression tree (BRT), support vector regression (SVR), random forest regression (RFR), lasso regression (LAR) and Bayesian additive regression trees (BART) 1,045 renal transplant patients Zarrinpar et al., 2016 [ 145 ] Individualizing liver transplant immunosuppression Second-order algebraic equation 8 patients Storset et al., 2015 [ 146 ] Improving Tacrolimus Target Concentration Computerized dosing 80 renal transplant recipients McMichael et al., 1991 [ 147 ] Optimizing FK 506 therapy “intelligent” dosing system (IDS) 32 patients Seeling et al., 2012 [ 148 ] Knowledge-based tacrolimus therapy for kidney transplant patients Regression tree patient data from 1995 to 2008 from the Department of Nephrology and Dialysis of the Vienna General Hospital Camps-Valls et al., 2003 [ 149 ] Personalizing the dosage of cyclosporine A Neural networks 22 patients Goren et al., 2008 [ 150 ] Predicting cyclosporine blood levels in renal transplantation patients Adaptive-network-based fuzzy inference system (ANFIS) 138 patients Woillard et al., 2021 [ 151 ] Estimate the concentration of MMF in transplant patients Extreme gradient boosting (Xgboost R package) ML models dataset of 12,877 MPA AUC requests from 6,884 patients

## Ethical and legal dimensions of AI in organ allocation and healthcare

The integration of AI and big data into healthcare, particularly in fields like organ transplant and allocation, presents both transformative opportunities and significant challenges. While AI technologies show promise in enhancing organ allocation procedures and patient results, they also spark worries about equity, openness, and responsibility [ 151 ]. Ethically, primary concerns include AI bias, patient consent, and data confidentiality. Organ allocation AI systems must be crafted to avoid reinforcing current inequalities and ensure fair organ access across varied patient groups [ 152 , 153 ]. Clarity in AI decision processes is vital for maintaining confidence and accountability in healthcare [ 154 ]. Moreover, the gathering and utilization of patient information for AI-driven organ allocation raise significant privacy issues that need to be tackled through strong data protection strategies. From a legal standpoint, incorporating AI in organ allocation faces hurdles related to accountability, efficacy and safety, and regulatory adherence. Well-defined structures are necessary to ascertain responsibility when AI-aided decisions result in negative outcomes [ 151 ]. Guaranteeing the safety and effectiveness of AI systems in healthcare demands stringent validation and continuous monitoring [ 153 ]. Additionally, the employment of AI in organ allocation must adhere to current healthcare regulations and data protection laws, which may require modification to address the unique challenges presented by AI technologies. To address these ethical and legal aspects, a cross-disciplinary strategy is crucial. This encompasses developing comprehensive ethical guidelines, encouraging collaboration among healthcare experts, data scientists, and ethicists, and implementing robust regulatory frameworks [ 155 ]. By proactively tackling these challenges, the medical community can leverage AI’s potential in organ allocation while upholding ethical principles and legal standards, ultimately striving for more efficient and equitable healthcare provision.

Safeguarding the ethical and legal aspects of AI-driven healthcare systems is critical, especially in protecting patient data privacy, ensuring informed consent, and respecting patient autonomy [ 156 ]. Data privacy becomes even more critical in organ allocation scenarios where donors and recipients remain anonymous. Leakage of sensitive patient information in these cases can lead to serious consequences, such as emotional distress, potential exploitation, and breaches of confidentiality agreements. For example, if a donor’s identity is exposed, it may lead to unwanted contact from recipients or their families, creating emotional and legal complications. Similarly, recipients may face stigma or undue pressure if their personal information is revealed. Therefore, maintaining strict confidentiality protocols is vital to ensure trust and the smooth functioning of the organ allocation process. As AI continues to influence healthcare and precision medicine, robust data protection legislation becomes vital. Laws like the Health Insurance Portability and Accountability Act (HIPAA) in the United States and the General Data Protection Regulation (GDPR) in Europe set global benchmarks for privacy [ 157 ]. HIPAA focuses on safeguarding health information from covered entities, whereas GDPR enforces comprehensive data protection standards across the EU, creating a ripple effect worldwide [ 158 ]. In the context of organ allocation, ethical risks amplify due to the sensitive nature of the data and the life-and-death decisions involved. An ethical assessment framework emphasizing transparency, accountability, and awareness has been proposed to guide the adoption of AI in healthcare. These frameworks ensure that stakeholders across the healthcare supply chain align with ethical priorities while navigating the complex interplay of technology, data, and human welfare [ 159 ].

## Real-world examples of successful AI applications in transplantation

The field of transplantation has seen encouraging advancements in AI applications, though many remain in the developmental phase rather than being widely used in clinical settings. Among the AI models introduced for donor-recipient matching, “Smart Match” stands out as a real-world application actively used in clinical practice. Developed by R. Deshpande, this AI-powered system leverages machine learning algorithms to enhance the accuracy of donor-recipient pairing, ultimately optimizing transplantation outcomes. By addressing challenges in donor-recipient compatibility, immunosuppression management, and post-operative care, Smart Match aims to reduce mismatches, lower waitlist mortality, and improve overall patient outcomes. Although relatively new, the system has demonstrated promising results, streamlining the allocation process and increasing the efficiency of organ transplantation. Early implementations suggest it has the potential to significantly improve transplant success rates while reducing wait times for patients in need [ 160 ].

The “Continuous Distribution Framework,” developed through a collaboration between the United Network for Organ Sharing (UNOS) and the Massachusetts Institute of Technology (MIT), is a groundbreaking AI-driven algorithm used in real-world organ allocation. This system evaluates all patient factors simultaneously, generating a unique weighted score for each organ candidate to ensure more equitable and efficient transplant decisions. Successfully implemented in the United States, this framework has revolutionized organ distribution by integrating factors such as medical urgency, waiting time, and geographical location into a single, comprehensive scoring system. Its ability to balance multiple variables has significantly improved fairness and efficiency in the allocation process, addressing longstanding challenges in organ transplantation [ 161 ].

Among the various models and algorithms developed to predict post-transplantation complications, the iBox algorithm stands out as a significant real-world success. Created by the Paris Transplant Group under the leadership of Dr. Alexandre Loupy, this algorithm offers a reliable method to forecast short-, medium-, and long-term outcomes for transplanted organs.

The iBox algorithm analyzes gene expression in kidney, heart, and lung grafts to evaluate both the risk of rejection and the overall health of the graft. Its accuracy and effectiveness have been validated through extensive clinical studies, solidifying its role in practical medical applications. By providing precise predictions, the iBox model enables healthcare providers to customize post-transplant care plans and proactively address potential complications. This innovative tool has revolutionized transplant care by improving long-term graft survival rates and enhancing patient outcomes, making it an invaluable asset in the field of organ transplantation [ 162 ].

These examples illustrate the successful integration of AI into clinical transplantation practices, demonstrating its potential to transform patient care and optimize transplantation outcomes. although AI demonstrates significant potential in transplantation, its practical applications remain limited. Successful examples include enhanced organ allocation algorithms and donor-recipient matching. However, for widespread adoption, these tools need to be integrated into electronic medical records, organ offer systems, and mobile devices [ 163 ]. The field is rapidly evolving, and AI is anticipated to play an increasingly significant role in improving personalized clinical decision-making in transplant medicine.

## Future research directions

The field of medical research is witnessing rapid advancements, with artificial intelligence (AI) offering promising opportunities to enhance transplant medicine. Two key areas where AI’s transformative potential is gaining recognition are improving long-term graft survival and addressing inequities in organ allocation. As we approach this technological shift, future studies must harness AI-powered predictive models and innovative approaches to ensure fair and effective healthcare outcomes.

The creation of AI-driven predictive models for long-term graft survival is an emerging focus in transplantation medicine, with the potential to significantly improve patient outcomes. Liver transplantation (LT) exemplifies how AI integration can optimize clinical decision-making. The intricate management of LT recipients, involving numerous demographic, clinical, and laboratory factors, provides an opportunity for machine learning and deep learning applications. Regarding new scientific approaches in laboratory science, we know that laboratory factors implication extend beyond diagnostics and research, playing a pivotal role in strengthening the healthcare system and providing vital analytical support to healthcare providers, including physicians, facilitating informed decision-making in the ever-evolving landscape of modern medicine [ 164 ]. These technologies can be utilized pre-transplant to enhance donor-recipient matching and improve candidacy decisions, thus reducing waitlist mortality and boosting post-transplant outcomes [ 165 ]. In liver transplantation, identifying early allograft dysfunction (EAD) is crucial for predicting graft and patient survival. A recent study proposed a standardized EAD definition based on specific postoperative laboratory analyses, revealing a 23.2% EAD incidence and a strong link to graft loss and patient mortality [ 166 ]. This objective criterion can serve as a valuable endpoint in translational studies aimed at elucidating the mechanisms underlying graft dysfunction, thereby informing the development of predictive models that incorporate AI methodologies. Similarly, uric acid (UA) levels in kidney transplantation have gained attention as a potential predictor of long-term graft survival. Studies indicate that mean UA levels during the initial six months post-transplant are independently associated with graft loss and function, suggesting that incorporating such biomarkers into AI-driven models could enhance predictive accuracy [ 167 ]. By utilizing large datasets and advanced analytical techniques, researchers can develop models that consider both traditional clinical variables and novel risk factors.

The need for improved predictive models is particularly evident in pediatric kidney transplantation, where the current Kidney Donor Profile Index (KDPI) inadequately predicts graft survival. Alternative indices, such as the Child Donor Index (CDI) and Adolescent Donor Index (ADI), have shown superior predictive capabilities, highlighting AI’s potential to refine donor allocation strategies [ 168 ]. These findings underscore the importance of developing AI-driven models that can integrate diverse data sources, including clinical, demographic, and novel biomarkers, to enhance long-term graft survival predictions.

The integration of AI in transplantation medicine shows promise for developing robust predictive models that can significantly improve long-term graft survival. By addressing the limitations of existing methodologies and incorporating a wider range of risk factors, researchers can enhance the personalization of treatment plans and ultimately improve patient outcomes in both liver and kidney transplantation.The potential of artificial intelligence (AI) to address disparities in organ allocation is increasingly recognized in contemporary healthcare research. One significant area of concern is the inequitable access to preemptive kidney transplantation, particularly among marginalized populations. Reese et al. [ 169 ] highlight that preemptive transplantation can significantly improve patient outcomes compared to those who undergo dialysis. However, systemic barriers, including stringent eligibility criteria and a first-come, first-served allocation model, disproportionately disadvantage Black patients. The authors propose that standardizing waitlisting criteria and enhancing clinician education could mitigate these disparities, suggesting that AI could play a role in streamlining these processes. In the context of organ allocation, geographic disparities have also emerged as a critical issue. Feigin et al. [ 170 ] discuss how traditional allocation models, which often rely on artificial regional boundaries, can hinder access for patients with the most urgent medical needs. They propose a continuous geographical allocation model that utilizes a gravity model to prioritize patients based on their condition and proximity to available organs. This innovative approach demonstrates how AI could enhance fairness in organ distribution by ensuring that those in critical need have access to organs from further distances, thereby improving overall outcomes.

## Discussion, perspective and conclusion

Organ transplantation has evolved significantly since its inception in 1954, transitioning from early challenges to becoming a standard treatment for end-stage organ diseases. While advancements in surgical techniques and immunosuppression have contributed to its success, challenges remain in organ allocation. Artificial Intelligence holds the potential to revolutionize organ allocation by predicting outcomes and optimizing donor selection, ultimately improving efficiency and equity in transplantation.

Integrating AI and machine learning (ML) into solid organ transplantation is a major medical advancement, addressing challenges like organ shortages and outcome prediction. In terms of predictive analysis, machine learning (ML) algorithms hold great promise for improving organ allocation by increasing the accuracy of donor-recipient matches, outstanding traditional methods [ 171 ]. By analyzing various factors and their relationships, ML can optimize the allocation process. In post-transplant care, ML models are advancing to predict personalized short-term outcomes, with future efforts focused on long-term predictions. Despite progress, incorporating more patient data into AI research is essential to further improve predictive accuracy.

On the other hand, ML has played a key role in medical image analysis, particularly in specialized areas like transplant pathology. Advances in AI promise to revolutionize organ selection, matching, and allocation. AI helps address the shortage of skilled pathologists by detecting complex patterns, even surpassing human experts in areas like diagnosing kidney allograft rejection. Digital pathology scanners enhance AI’s impact, and accurate algorithms are being developed to identify organ rejection and assess organ quality, especially in liver biopsies. These advancements could improve the use of marginal organs, significantly enhancing the organ transplantation process.

The implementation of artificial intelligence (AI) in clinical settings, particularly medical imaging, faces several significant challenges. One primary obstacle is regulatory compliance, which requires demanding performance assessments to ensure that AI algorithms are generalizable and trustworthy. The complexities surrounding regulatory frameworks can impede the timely integration of AI tools into clinical practice [ 172 ]. Additionally, the training requirements for health care professionals pose another challenge, as practitioners must be adequately educated on the use and limitations of AI technologies to raise trust and effective utilization. This training is essential to mitigate skepticism among healthcare providers regarding AI’s reliability and efficacy [ 173 ]. Furthermore, the infrastructure needed to support AI implementation is often lacking, and healthcare facilities must invest in both technological upgrades and data management systems to accommodate AI tools effectively. This includes addressing technical hurdles related to data collection and algorithmic advancements that are crucial for the successful deployment of AI in clinical workflows. Overcoming these challenges requires a collaborative approach involving stakeholders across the healthcare spectrum, emphasizing the need for ongoing validation of AI tools in real-world settings to demonstrate their benefits for patient outcomes.

The true power of AI in transplantation care lies in its ability to integrate various aspects of medical practice into a cohesive whole. he integration of AI in radiology and pathology can improve workflow efficiency and diagnostic accuracy, allowing for faster and more precise analysis of medical images and tissue samples [ 174 ]. This enhanced diagnostic capability can provide surgeons with more comprehensive and timely information for transplantation decision-making. AI tools can assist radiologists in detecting subtle imaging findings and pathologists in analyzing complex histopathological patterns, potentially leading to earlier detection of complications or rejection [ 174 , 175 ]. Interestingly, while AI shows promise in improving diagnostic capabilities, there are concerns about its impact on professional roles. However, studies suggest that AI is unlikely to replace radiologists or pathologists, but rather augment their capabilities and allow them to focus on higher-value tasks [ 176 ]. This shift may create opportunities for increased direct involvement of radiologists and pathologists in multidisciplinary clinical teams, fostering closer collaboration with surgeons. AI has the potential to serve as a unifying platform for multidisciplinary collaboration in transplantation care. By streamlining workflows, enhancing diagnostic accuracy, and facilitating data sharing, AI can enable more effective communication and decision-making among surgeons, radiologists, and pathologists. However, successful implementation will require addressing challenges such as data integration, ethical considerations, and the need for specialized training to ensure all team members can effectively utilize AI tools in their collaborative efforts [ 175 , 177 , 178 ].

Another aspect of the discussion in this area is while the integration of AI and machine learning in organ transplantation has shown promising advancements, yet the implementation and effectiveness of these technologies vary across healthcare systems globally. AI use in the field of organ transplantation varies significantly across different health systems worldwide. This variation is due to the technological infrastructure, regulatory frames, and societies’ attitudes toward adopting AI in managing human life [ 16 ]. Comparative insights bring to light both best practices and challenges that may inform future developments in this domain. As discussed above, one of the primary areas where AI is making an impact is in organ allocation and donor-recipient matching. There is indeed a growing optimism among transplant clinicians in the United States that these processes might be integrated with AI to provide allocation systems not only more equitable but also efficient in their function [ 179 , 180 ]. However, ethical considerations, such as algorithmic bias and transparency, remain significant challenges that need to be addressed to ensure fair outcomes [ 160 , 181 ]. The use of AI in transplantation differs greatly across the different countries. It is widespread and highly advanced in Western Europe and North America, in regions with well-developed healthcare systems. These areas are using big data and very sophisticated techniques of machine learning to improve both the pre-transplant workup and the post-transplant surveillance [ 163 ]. While in developing countries, the implementation of AI is usually restricted by resources and infrastructure that limit the successful implementation of modern technologies [ 182 , 183 ]. This disparity highlights the need for tailored approaches that consider local contexts and capabilities when integrating AI into transplantation practices. Despite the promising developments, there are still many obstacles to overcome in the global AI field of organ transplantation. Issues such as data privacy, the need for robust validation of AI models, and the integration of AI into clinical workflows are critical hurdles that must be overcome [ 160 , 181 ]. This, however, also creates an urgent need for collaboration across disciplinary boundaries between clinicians, data scientists, and ethicists to develop effective, yet ethically viable AI applications.

In conclusion, while AI and ML algorithms show great promise in enhancing predictive capabilities in organ transplantation, challenges remain in their clinical implementation. Future research should focus on improving model interpretability, validating algorithms across diverse populations, and addressing data quality issues to fully realize the potential of AI in transplantation medicine. All the discussed technologies can uncover patterns in transplantation that traditional methods might miss. However, accurate long-term outcome predictions are insufficient, highlighting the need for future studies. The combination of AI and solid organ transplantation holds great potential. As technology advances and extensive datasets become available, AI-driven insights in transplantation are emerging as a promising area for future research and innovation.

### Table: Different AI models used in organ transplant

| Category | AI Model | Brief explanation |
|---|---|---|
| Neural Networks & Deep Learning | Deep Learning | a subset of machine learning that focuses on using neural networks with many layers (deep neural networks) to model and understand complex patterns in data. It is particularly powerful for tasks. |
| Deep Neural Network (DNN) | a type of artificial neural network with multiple layers of neurons between the input and output layers. These layers are known as hidden layers, and the depth of the network refers to the number of these hidden layers. DNNs are a subset of machine learning models and are a key component of deep learning, which is a branch of artificial intelligence |
| DeepSurv | a deep learning-based approach for survival analysis, specifically designed to predict the time to an event (such as failure or death) and the associated risk based on complex, high-dimensional data. It extends traditional survival models by leveraging the power of deep neural networks to handle complex relationships and interactions between predictors |
| Generative Adversarial Networks (GAN) | a type of deep learning architecture used for generating new data samples that resemble a given dataset. Introduced by Ian Goodfellow and his colleagues in 2014, GANs consist of two neural networks that compete with each other in a process that drives them to improve over time. |
| Multilayer Perceptron (MLP) | a type of artificial neural network used for various machine learning tasks, including classification and regression. It consists of multiple layers of nodes (neurons) arranged in a layered architecture. Each layer performs a specific function, and the network learns by adjusting weights through training |
| Ensemble Methods | Ensemble Learning | By merging multiple decision trees into a unified model, the strengths and weaknesses of individual predictions are effectively utilized |
| Random Forest | A collection of independent decision trees with superior accuracy compared to the cumulative accuracy of each individual tree |
| AdaBoost | AdaBoost (Adaptive Boosting) is a popular ensemble learning technique designed to improve the performance of machine learning models by combining the predictions of multiple weak learners to create a strong classifier. |
| Random Survival Forests | a machine learning technique used for analyzing and predicting time-to-event data, often referred to as survival data. This method is an extension of the Random Forest algorithm, which is commonly used for classification and regression tasks |
| Extreme Gradient Boosting (XGBoost) | a highly efficient and scalable machine learning algorithm for supervised learning problems, particularly for classification and regression tasks. XGBoost is known for its ability to handle large datasets and its high predictive accuracy. |
| Traditional Machine Learning | Support Vector Machine (SVM) | are a powerful and versatile class of supervised learning algorithms primarily used for classification tasks, but they can also be applied to regression problems. |
| Logistic Regression | a statistical model used for binary classification tasks. Despite its name, logistic regression is used for classification rather than regression. It estimates the probability of a binary outcome based on one or more predictor variables. |
| Multivariable Logistic Regression | Multivariable Logistic Regression (often referred to as Multivariate Logistic Regression or simply Logistic Regression with multiple predictors) is an extension of logistic regression that includes more than one predictor variable. |
| Cox Proportional Regression | Cox Proportional-Hazards Regression (often simply called Cox Regression) is a statistical method used for analyzing survival data and understanding the effect of explanatory variables on the time until an event occurs. Developed by Sir David Cox in 1972, it is widely used in medical research, epidemiology, and other fields dealing with time-to-event data. |
| Survival Tree | type of decision tree specifically designed for analyzing survival data, which involves time-to-event outcomes. They are used to predict the time until an event occurs (such as failure, death, or relapse) based on various predictor variables. |
| Memetic Pareto Evolutionary NSGA-II | an advanced multi-objective optimization algorithm. It combines the strengths of memetic algorithms and NSGA-II to efficiently solve complex optimization problems involving multiple conflicting objectives. |
| MoRAL-AI | MoRAL-AI (Model of Responsibility for AI) is a framework designed to address the ethical and responsible development and deployment of artificial intelligence systems. It focuses on integrating ethical considerations and responsible practices into AI models and processes |
| Optimization & Evolutionary Algorithms | Adaptive-network-based Fuzzy Inference System | a type of artificial neural network that combines fuzzy logic with neural network learning capabilities. ANFIS is used for modeling complex, non-linear systems and can be employed in various applications such as prediction, classification, and control |
| Decision Tree | a popular machine learning and statistical technique used for classification and regression tasks. They represent a model that makes decisions based on a series of binary or categorical splits in the data, visualized as a tree structure. Each node in the tree represents a decision based on a feature, and each branch represents the outcome of that decision |
| Fuzzy Logic & Inference Systems | Conditional Inference Tree | a type of decision tree designed to avoid some common issues in traditional decision tree algorithms, such as bias towards variables with more categories or continuous variables. They use statistical tests to determine the best split points, ensuring that splits are selected based on statistical significance rather than just algorithmic criteria. |
| Decision Trees & Pruning Methods | Case-Based Reasoning | an approach within artificial intelligence and machine learning where new problems are solved based on the solutions of similar past problems |
| Case Retrieval Nets | a sophisticated method used in case-based reasoning (CBR) to efficiently retrieve relevant cases from a large case base. |
| Case-Based Reasoning & Retrieval | Spreading Activation Algorithm | a family of techniques used in artificial intelligence, cognitive science, and information retrieval to find and rank related concepts or nodes within a network. These algorithms simulate how human memory might work, by “activating” certain nodes and allowing this activation to spread to related nodes, thereby identifying relevant or related information. |
| Reinforcement Learning | Enhances the likelihood of attaining a goal within a specific circumstance or setting. |
| Finite Impulse Response (FIR) | a type of digital filter used in signal processing. FIR filters are characterized by their response to an impulse input that settles to zero in a finite amount of time. They are widely used due to their stability and linear phase properties. |
| Reinforcement Learning | Lund DLTransplant Algorithm | a specific set of criteria and scoring system used to prioritize patients for liver transplantation. It is particularly focused on pediatric liver transplants and aims to optimize outcomes by effectively allocating liver donations based on the urgency of need and potential benefit. |
| Signal Processing | MUSA-UNet | an advanced variation of the UNet architecture specifically designed for medical image segmentation tasks. It incorporates several enhancements to improve performance and accuracy, particularly in challenging medical imaging scenarios |
| Specialized Models | Binary Thresholding | a simple image processing technique used to segment an image into two distinct regions based on pixel intensity values. It is commonly used in various applications, including image binarization, object detection, and feature extraction. |
| Multiple Regression Models | a statistical technique used to understand the relationship between one dependent variable and two or more independent variables. This method extends simple linear regression, which involves only one independent variable, to analyze and predict the dependent variable based on the combined influence of several predictors |


### Table: Comparison between AI-based prediction models and traditional clinical scoring systems

| Features of prediction method | AI-Based Models | Traditional Models |
|---|---|---|
| Example Models | Artificial Neural Networks (ANNs), Random Forest, XGBoost, DeepSurv | MELD (Model for End-Stage Liver Disease), SOFA (Sequential Organ Failure Assessment), APACHE (Acute Physiology and Chronic Health Evaluation) |
| Prediction Accuracy | Higher accuracy due to deep learning and large dataset analysis | Lower accuracy, relies on predefined clinical scores |
| Data Processing | Can analyze high-dimensional data (genomics, imaging, EHRs, biometrics) | Limited to structured clinical variables and statistical models |
| Personalization | Tailors predictions based on patient-specific features using adaptive learning | More generalized risk assessment using population-based scores |
| Real-Time Adaptability | Continuously updates with new data and improves over time | Static models that require manual recalibration |
| Explainability | Often criticized as a “black box”; some models (e.g., SHAP-based) improve interpretability | More transparent and widely understood by clinicians |
| Integration with Clinical Workflow | Requires advanced IT infrastructure and clinician training | Already integrated into routine clinical decision-making |
| Regulatory Approval & Validation | Needs extensive validation (e.g., FDA approval) due to complexity and variability | Established and widely accepted in practice |
| Cost & Implementation | High initial investment in AI infrastructure, expertise, and data management | Lower cost, as traditional models are already in use |


### Table: Summary of studies using AI for organ transplant

| Author, Year | Purpose | AI-model | dataset |
|---|---|---|---|
| Giuste et al., 2023 [ 72 ] | Enhancing risk assessment of rare pediatric heart transplant rejection through the generation of synthetic images | Progressive and Inspirational GAN | 12 non rejection and 12 rejection slides |
| Lipkova et al., 2022 [ 74 ] | Assessment of cardiac allograft rejection from endomyocardial biopsies | Pre-trained deep residual CNN | Training: 1352 WSI slides; Validation: 1840 WSI slides |
| Seraphin et al., 2023 [ 73 ] | Evaluated the feasibility of using deep learning in predicting the degree of cellular rejection from pathology slides and predict heart transplant rejection from routine pathology slides | attention-based deep neural network | 1079 histopathology slides from 325 patients from three transplant centres in Germany |
| Hsich et al., 2019 [ 53 ] | Variables of importance in the Scientific Registry of Transplant Recipients database predictive of heart transplant waitlist mortality | RF model | 33,069 waitlist |
| Medved et al., 2018 [ 54 ] | Simulating the outcome of heart allocation policies using deep neural networks | Neural network-based algorithm named Lund Deep Learning Transplant Algorithm (LuDeLTA) | 30,584 recipients and 18,982 donors |
| Ayllón et al., 2018 [ 55 ] | Validation of artificial neural networks as a methodology for donor–recipient matching for liver transplantation | ANN mode | 822 donor–recipient pairs |
| Moreno et al., 2017 [ 56 ] | Solving an imbalanced liver transplantation problem | Ordinal neural network | 634 donor–recipient pairs |
| Placona et al., 2020 [ 57 ] | Predict the delay or discard of adult deceased donors based on donor-free-text data | Natural Language Processing mode | 74,041 donors |
| Bricen˜o et al., 2014 [ 64 ] | Use of artificial intelligence as an innovative donor-recipient matching model for liver transplantation | Multiple regression models, ANN, Neural Net Evolutionary Programming | utilized 64 donor and recipient variables from 1003 liver transplants across 11 Spanish centers |
| Cruz-Ramírez et al., 2013 [ 17 ] | Predicting patient survival after liver transplantation | Evolutionary multi-objective artificial neural networks | dataset of liver transplants collected by eleven Spanish hospitals |
| Furness et al., 1999 [ 67 ] | Assist in the histological diagnosis of early acute renal allograft rejection | Neural network | transplant biopsies from Oxford |
| Tong et al., 2017 [ 68 ] | Predicting heart rejection | Deep neural network | histopathological whole-slide imaging of 43 patient |
| Hermsen et al., 2019 [ 69 ] | Histopathologic Assessment of Kidney Tissue | Convolutional neural network | 40 whole-slide images of stained kidney transplant biopsies |
| Perez-Sanz et al., 2021 [ 70 ] | Efficiency of machine learning algorithms for the determination of macrovesicular steatosis in frozen sections stained with Sudan to evaluate the quality of the graft in liver transplantation | Machine learning and artificial vision | 20 donor liver samples |
| Gholamzadeh et al., 2022 [ 77 ] | Improve lung transplantation outcomes and complications | Support vector machine, logistic regression, Random Forests, Bayesian network, Markov Model, KNN, k- means, Gradient Boosting trees (XGBoost), Convolutional Neural Network | electronic databases from January 2000 to June 2022 |
| Davis et al, 2020 [ 78 ] | Detection of ACR in lung transplant biopsies | AI model | 3349 annotations |
| Smith et al., 2023 [ 85 ] | Quantifying the amount of non-glomerular inflammation within the cortex | A UNet model | 60 biopsies from 53 patients |
| Hermsen et al., 2022 [ 86 ] | Quantifying the chronic and inflammatory lesions in kidney transplant biopsies | A UNet model | 125 WSI pairs of periodic acid-schiffand CD3-stained slides |
| Kers et al., 2022 [ 88 ] | Classifying histology of kidney allograft biopsies | Single and serial CNN | 5844 WSIs from 1948 patients |
| Zhengzi Yi et al., 2022 [ 87 ] | Characterizing pathological abnormalities in kidney transplant biopsies | DL model | slide images from 789 transplant biopsies |
| Narayan et al., 2022 [ 90 ] | Prediction of donor liver allograft steatosis and early post-transplantation graft failure | CVAI model consisting of Fully Convolutional Networks (FCN) and UNet | 25,494 images from 90 liver biopsies |
| Yu et al., 2022 [ 91 ] | Segmentation of portal tract regions from whole-slide images of liver tissue biopsies | MUSA-UNet, FCN, UNet, DeepLab V3 | 53 WSIs (Training and Validation: 30; Testing: 20) |
| Sun et al., 2022 [ 92 ] | Quantify percent steatosis in donor liver biopsy frozen sections | Pre-trained VGG16 (Truncated at bottleneck layer) | 96 WSIs (Training: 30; Testing: 66) |
| Luo et al., 2020 [ 100 ] | Identifying kidney transplant patients at higher risk of severe pneumonia during their post-transplant hospital stay | Random Forest, AdaBoost, support vector machines, Naïve Bayes, and logistic regression | five hundred nineteen patients who underwent transplantation from January 2015 to December 2018 were included |
| Irish et al., 2010 [ 106 ] | Predicting DGF following renal transplantation | Multivariable logistic regression analysis | 24,337 deceased donor renal transplant recipients between 2003 and 2006 |
| Senanayake et al., 2019 [ 108 ] | Predicting graft failure in live and deceased donor kidney transplants | Survival tree, random survival forest, and survival support vector machine, Cox proportional regression | large national dataset from Australia and New Zealand Dialysis and Transplant Registry |
| Kawakita et al., 2020 [ 109 ] | Evaluate various machine learning effectiveness in predicting DGF in kidney transplant recipients | Logistic regression, elastic net, random forest, artificial neural network, and extreme gradient boosting | data from adult DDKT recipients for model development ( n = 55,044) and validation ( n = 6176). |
| Agasthi et al., 2020 [ 110 ] | A risk prediction for survival and graft failure (GF) five years post-orthotopic heart transplant (OHT) | Gradient-boosted machine (GBM) model | 15,236 patients from January 2005 to December 2009 |
| Shaikhina et al., 2019 [ 118 ] | Predicting acute antibody-mediated rejection (AMR) 30 days post-transplant | Decision tree and random forest | 80 kidney transplants |
| Tapak et al., 2017 [ 117 ] | Identify potential essential risk factors for chronic nonreversible graft rejection | Logistic regression, ANN | 378 patients from Hamadan, Western Iran, from 1994 to 2011 |
| Esteban et al., 2016 [ 119 ] | Predicting acute rejection in kidney transplant | Recurrent neural network (RNN) | database collected in the Charité Hospital in Berlin |
| Suthanthiran et al., 2013 [ 120 ] | Prediction of Urinary-cell mRNA profile and acute cellular rejection in kidney allografts | Logistic regression | 4300 urine specimens from 485 kidney-graft recipients from day 3 through month 12 after transplantation |
| Zare et al., 2017 [ 116 ] | Early predicting acute allograft rejection after transplantation in liver transplant recipients | ANN | data from 148 liver transplant recipients over days 3, 7, and 14 post-transplantations |
| Abdeltawab et al., 2019 [ 121 ] | Early Assessment of Transplanted Kidney Dysfunction | Convolutional neural network | scans collected from 56 subjects from geographically diverse populations |
| Reeve et al., 2017, 2019 [ 122 , 123 ] | Assessing rejection-related disease in kidney transplant biopsies with the aid of automated kidney transplant biopsy reports | Clustering analysis | 1,208 kidney biopsies |
| Williams et al., 2017 [ 126 ] | Identify protein biomarkers of delayed recovery from kidney transplant. | Random Forest analysis | 42 patients |
| Daher Costa et al., 2020 [ 127 ] | Evaluated the risk factors for delayed graft function (DGF) | Retrospective analysis | 443 kidney transplants from brain-dead deceased donors in two Brazilian centers |
| Villeneuve et al., 2016 [ 128 ] | Assess the health-related quality of life after kidney transplant | Random forest | 337 kidney transplant recipients during the first three years post-transplantation |
| Moccia et al., 2018 [ 129 ] | Assessment of graft steatosis in liver transplantation | Semi-supervised classification technique | 40 liver images |
| Bhat et al., 2018 [ 130 ] | Identifying key predictors and survival outcomes of new-onset diabetes after transplant in liver transplant recipients | Various machine learning approaches | data from the Scientific Registry of Transplant Recipients, between October 1, 1987, and March 31, 2016 |
| Tanaka and Voigt, 2018 [ 131 ] | Risk Evaluation of developing non-melanoma skin cancers in liver transplant recipients | Decision tree | 102,984 cases |
| Lee et al., 2019 [ 132 ] | Prediction the risk of persistent alcohol consumption following a liver transplant in patients with alcohol hepatitis | The LASSO regression method | 134 liver transplant recipients |
| Lee et al., 2018 [ 133 ] | Evaluation of the effectiveness of ML approaches and logistic regression analysis in predicting AKI after liver transplantation | Decision tree, random forest, gradient boosting machine, support vector machine, naïve Bayes, multilayer perceptron, and deep belief networks | 1211 patients |
| He et al., 2021 [ 134 ] | Predicting acute kidney injury following donation after cardiac death liver transplantation | Random forest, support vector machine, classical decision tree, and conditional inference tree | 493 recipients |
| Chen et al., 2021 [ 135 ] | Predict pneumonia after liver transplantation | Many novel ML models | 786 adult patients |
| Nam et al., 2020 [ 136 ] | Forecast the recurrence of hepatocellular carcinoma after liver transplantation | Deep neural network (MoRAL-AI) | 563 patients |
| Ivanics et al., 2022 [ 137 ] | Assessing the risk of hepatocellular carcinoma recurrence following liver transplantation | CoxNet (regularized Cox regression), survival random forest, survival support vector machine, and DeepSurv | 739 patients |
| Jain et al., 2021 [ 138 ] | Forecasting Major Adverse Cardiovascular Events following Orthotopic Liver Transplantation | Retrospective cohort study design | 1,459 consecutive patients |
| Nitski et al., 2021 [ 139 ] | Predicting complications leading to mortality after liver transplantation | ANN | 3,269 patients |
| Barbosa Jr et al., 2018 [ 140 ] | Assessing early detection for Chronic Lung Allograft Dysfunction | Support Vector Machine | 71 patients |
| Sadat Hosseini-Baharanchi et al., 2016 [ 141 ] | Evaluate the occurrence of Bronchiolitis Obliterans Syndrome-related deaths in Lung Transplant Recipients | Retrospective cohort study | 44 recipients |
| Thishya et al., 2018 [ 21 ] | predicting the bioavailability of tacrolimus in patients with renal transplantation | Artificial neural network (ANN) and logistic regression (LR) models | blood samples were collected from 54 patients on day 1, 3, 5, 10, 15, 30, 45, 60 and 90 day post-transplantation |
| Tang et al., 2017 [ 19 ] | Predicting Tacrolimus Stable Dose in Renal Transplant Recipients | Multiple linear regression (MLR), artificial neural network (ANN), regression tree (RT), multivariate adaptive regression splines (MARS), boosted regression tree (BRT), support vector regression (SVR), random forest regression (RFR), lasso regression (LAR) and Bayesian additive regression trees (BART) | 1,045 renal transplant patients |
| Zarrinpar et al., 2016 [ 145 ] | Individualizing liver transplant immunosuppression | Second-order algebraic equation | 8 patients |
| Storset et al., 2015 [ 146 ] | Improving Tacrolimus Target Concentration | Computerized dosing | 80 renal transplant recipients |
| McMichael et al., 1991 [ 147 ] | Optimizing FK 506 therapy | “intelligent” dosing system (IDS) | 32 patients |
| Seeling et al., 2012 [ 148 ] | Knowledge-based tacrolimus therapy for kidney transplant patients | Regression tree | patient data from 1995 to 2008 from the Department of Nephrology and Dialysis of the Vienna General Hospital |
| Camps-Valls et al., 2003 [ 149 ] | Personalizing the dosage of cyclosporine A | Neural networks | 22 patients |
| Goren et al., 2008 [ 150 ] | Predicting cyclosporine blood levels in renal transplantation patients | Adaptive-network-based fuzzy inference system (ANFIS) | 138 patients |
| Woillard et al., 2021 [ 151 ] | Estimate the concentration of MMF in transplant patients | Extreme gradient boosting (Xgboost R package) ML models | dataset of 12,877 MPA AUC requests from 6,884 patients |


## Figures

**Figure:** Applications of AI in the realm of solid organ transplant

![](An_In-depth_overview_of_artificial_intelligence_AI_tool_utilization_across_diverse_phases_of_organ_t/figures/12967_2025_6488_Fig1_HTML.jpg)


**Figure:** Using artificial intelligence in the area of donor recipient matching considerations and decision making

![](An_In-depth_overview_of_artificial_intelligence_AI_tool_utilization_across_diverse_phases_of_organ_t/figures/12967_2025_6488_Fig2_HTML.jpg)


**Figure:** Using machine learning models for interpreting the date from Endomyocardial biopsy (EMB) slides, both pre and post-transplant, for establishing cardiac allograft rejection

![](An_In-depth_overview_of_artificial_intelligence_AI_tool_utilization_across_diverse_phases_of_organ_t/figures/12967_2025_6488_Fig3_HTML.jpg)



### Semantic Scholar Abstract

Artificial Intelligence (AI) offers a revolutionary approach to improve decision-making in medicine through the use of advanced computational tools. Its ability to analyze large and complex datasets enables a thorough evaluation of multiple factors, leading to a deeper understanding of medical procedures. Numerous studies have demonstrated that AI has made significant advancements in areas such as organ allocation, donor-recipient matching, and immunosuppression protocols in organ transplantation. The transplantation process consists of three key stages: pre-transplant evaluation, the surgical procedure, and post-transplant management. AI can enhance all three stages by analyzing and integrating data from histopathological reports, lab results, radiological features, and patient demographics to aid in matching donors and recipients. Additionally, AI supports robotic-assisted surgery and optimizes post-transplant regimens while evaluating complications. Various researches have utilized machine learning (ML) to predict medication bioavailability immediately after transplantation and assess the risk of post-transplant complications based on factors like genetic phenotypes, age, gender, and body mass index. This review aims to gather information on AI applications across various stages of organ transplantation and elaborate the strategies and tools relevant to these processes. Graphical Abstract
